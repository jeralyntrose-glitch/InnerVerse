import os
import uuid
import io
import base64
import json
import httpx
import csv
import tempfile
import subprocess
from datetime import datetime, timezone, timedelta
from collections import deque
from fastapi import FastAPI, UploadFile, File, Request, Header, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.openapi.docs import get_swagger_ui_html
from pydantic import BaseModel
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai
from pinecone import Pinecone
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.enums import TA_LEFT
from pydub import AudioSegment
import psycopg2
from psycopg2.extras import RealDictCursor
import pytz
from youtube_transcript_api import YouTubeTranscriptApi
import re
import requests
from http.cookiejar import MozillaCookieJar

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX = os.getenv("PINECONE_INDEX")
DATABASE_URL = os.getenv("DATABASE_URL")

# === Startup Logging ===
print("üöÄ Starting InnerVerse...")
print(f"‚úÖ OPENAI_API_KEY: {'SET' if OPENAI_API_KEY else 'MISSING'}")
print(f"‚úÖ PINECONE_API_KEY: {'SET' if PINECONE_API_KEY else 'MISSING'}")
print(f"‚úÖ PINECONE_INDEX: {'SET' if PINECONE_INDEX else 'MISSING'}")
print(f"‚úÖ DATABASE_URL: {'SET' if DATABASE_URL else 'MISSING'}")

# === YouTube Cookies Helper ===
def get_youtube_session_with_cookies():
    """Create a requests session with YouTube cookies loaded"""
    session = requests.Session()
    cookies_file = "youtube_cookies.txt"
    
    if os.path.exists(cookies_file):
        try:
            cookie_jar = MozillaCookieJar(cookies_file)
            cookie_jar.load(ignore_discard=True, ignore_expires=True)
            session.cookies = cookie_jar
            print(f"‚úÖ Loaded YouTube cookies for transcript API")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load cookies: {e}")
    else:
        print(f"‚ö†Ô∏è youtube_cookies.txt not found - FREE transcripts may be blocked by YouTube")
    
    return session

# === Usage Tracking ===
usage_log = deque(maxlen=1000)  # Keep last 1000 API calls
request_timestamps = deque(maxlen=1000)  # For rate limiting

# Pricing per 1K tokens (as of Oct 2025)
PRICING = {
    "text-embedding-ada-002": 0.0001,  # per 1K tokens
    "gpt-3.5-turbo-input": 0.0005,     # per 1K tokens
    "gpt-3.5-turbo-output": 0.0015,    # per 1K tokens
    "whisper-1": 0.006,                # per minute of audio
}

# === Database Functions ===
def get_db_connection():
    """Get PostgreSQL database connection"""
    if not DATABASE_URL:
        print("‚ö†Ô∏è DATABASE_URL not set - cost tracking will not work")
        return None
    return psycopg2.connect(DATABASE_URL)

def init_database():
    """Initialize database tables for API usage tracking"""
    try:
        conn = get_db_connection()
        if not conn:
            return
        cursor = conn.cursor()
        
        # Create api_usage table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS api_usage (
                id SERIAL PRIMARY KEY,
                timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                operation VARCHAR(100) NOT NULL,
                model VARCHAR(100) NOT NULL,
                input_tokens INTEGER DEFAULT 0,
                output_tokens INTEGER DEFAULT 0,
                cost DECIMAL(10, 6) NOT NULL
            )
        """)
        
        # Create index on timestamp for faster queries
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_api_usage_timestamp 
            ON api_usage(timestamp DESC)
        """)
        
        conn.commit()
        cursor.close()
        conn.close()
        print("‚úÖ Database initialized successfully")
    except Exception as e:
        print(f"‚ùå Database initialization error: {str(e)}")

def log_api_usage(operation, model, input_tokens=0, output_tokens=0, cost=0.0):
    """Log API usage with timestamp and cost to database"""
    try:
        conn = get_db_connection()
        if not conn:
            return
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO api_usage (operation, model, input_tokens, output_tokens, cost)
            VALUES (%s, %s, %s, %s, %s)
        """, (operation, model, input_tokens, output_tokens, round(cost, 6)))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f"üí∞ API Usage: {operation} | Model: {model} | Cost: ${cost:.6f}")
    except Exception as e:
        print(f"‚ùå Failed to log API usage: {str(e)}")
        # Fallback to in-memory logging if database fails
        usage_log.append({
            "timestamp": datetime.now().isoformat(),
            "operation": operation,
            "model": model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": round(cost, 6)
        })

def check_rate_limit(max_requests_per_hour=100):
    """Check if rate limit is exceeded"""
    now = datetime.now()
    one_hour_ago = now - timedelta(hours=1)
    
    # Remove old timestamps
    while request_timestamps and datetime.fromisoformat(request_timestamps[0]) < one_hour_ago:
        request_timestamps.popleft()
    
    # Check limit
    if len(request_timestamps) >= max_requests_per_hour:
        return False, len(request_timestamps)
    
    # Add current timestamp
    request_timestamps.append(now.isoformat())
    return True, len(request_timestamps)

# Initialize clients
def get_openai_client():
    openai.api_key = OPENAI_API_KEY
    return openai


def get_pinecone_client():
    if not PINECONE_API_KEY or not PINECONE_INDEX:
        return None
    pc = Pinecone(api_key=PINECONE_API_KEY)
    return pc.Index(PINECONE_INDEX)


# Split PDF text into chunks
def chunk_text(text, chunk_size=1000, chunk_overlap=200):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,
                                              chunk_overlap=chunk_overlap)
    chunks = splitter.split_text(text)
    return chunks


# Load InnerVerse taxonomy schema
def load_innerverse_schema():
    """Load the MBTI/Jungian taxonomy schema for auto-tagging"""
    try:
        with open('innerverse_schema.json', 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print("‚ö†Ô∏è innerverse_schema.json not found - auto-tagging disabled")
        return None


# Auto-tag document using GPT and InnerVerse schema
async def auto_tag_document(text, filename, openai_client):
    """
    Analyze document content and extract MBTI/Jungian taxonomy tags using GPT.
    Returns a list of relevant tags from the InnerVerse Intelligence Layer.
    """
    schema = load_innerverse_schema()
    if not schema:
        return []
    
    # Sample first 3000 chars for analysis (balance cost vs accuracy)
    sample_text = text[:3000] if len(text) > 3000 else text
    
    # Build prompt with schema context
    prompt = f"""You are an expert in MBTI, Jungian psychology, and cognitive functions. Analyze this document and extract ALL relevant taxonomy tags from the InnerVerse Intelligence Layer schema.

Document Title: {filename}
Document Sample: {sample_text}

TAXONOMY SCHEMA (6 Layers):

1. COGNITIVE ARCHITECTURE
- Cognitive Functions: Fe, Fi, Te, Ti, Ne, Ni, Se, Si
- Function Axes: Fe-Ti, Te-Fi, Ne-Si, Ni-Se
- Cognitive Roles: Hero, Parent, Child, Inferior, Nemesis, Critic, Trickster, Demon
- Cognitive Polarities: Thinking vs Feeling, Intuition vs Sensing, Judging vs Perceiving

2. TYPOLOGICAL STRUCTURES
- Four Sides of Mind: Ego, Subconscious, Unconscious, Shadow, Superego
- Interaction Styles: Directing, Initiating, Responding, Informing
- Temperaments: Artisan, Guardian, Idealist, Rational
- Quadras: Alpha, Beta, Gamma, Delta
- Loops & Grips: Ne-Fi loop, Ti-Ne loop, Se-Ni grip, etc.

3. TYPE-SPECIFIC INDEXING
- MBTI Types: INTJ, INTP, ENTJ, ENTP, INFJ, INFP, ENFJ, ENFP, ISTJ, ISFJ, ESTJ, ESFJ, ISTP, ISFP, ESTP, ESFP
- Type Pair Dynamics: Golden Pair, Silver Pair, Shadow Pair
- Intertype Relations: Duality, Conflict, Mirage, Supervision, Activation
- Compatibility Themes: Love Languages, Empathy Mismatches
- Developmental Stages: Childhood Imprinting, Midlife Individuation

4. DEPTH PSYCHOLOGY & JUNGIAN
- Archetypes: Hero, Anima, Animus, Shadow, Self, Trickster
- Shadow Integration, Individuation Process
- Complexes: Mother Complex, Father Complex, Inferiority Complex
- Dream Symbolism, Active Imagination

5. BEHAVIORAL EXPRESSION
- Communication Styles: Fe Harmony Speech, Ti Precision Speech, etc.
- Emotional Regulation: Fi Internal Processing, Fe External Harmony
- Conflict Resolution, Energy Management
- Shadow Triggers & Defense Mechanisms

6. INTEGRATION & META
- Function Interaction Maps: Fe ‚Üí Ne, Ti ‚Üí Si, etc.
- Cross-System Overlays: MBTI + Enneagram, MBTI + Temperament
- Type Evolution: Type Maturation, Age-Based Development
- Consciousness Levels, Metaphorical Frameworks

INSTRUCTIONS:
1. Read the document sample carefully
2. Identify ALL relevant tags from the taxonomy above
3. Return ONLY short tag names - NO category prefixes (e.g., "Sexual Compatibility" NOT "Compatibility Themes: Sexual Compatibility")
4. Return ONLY a JSON array of tag strings, nothing else
5. Be thorough - include any tag that's clearly discussed in the document
6. Format: ["tag1", "tag2", "tag3"]

CORRECT examples:
["Fe", "ENFJ", "Fe-Ti", "Golden Pair", "Shadow Integration", "Sexual Compatibility", "Emotional Compatibility"]

WRONG examples (DO NOT DO THIS):
["Compatibility Themes: Sexual Compatibility", "Developmental Stages: Relationship Success"]

Your response (JSON array only):"""

    try:
        print(f"üè∑Ô∏è Auto-tagging document: {filename}")
        
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are an expert MBTI/Jungian analyst. Extract taxonomy tags from documents."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        # Track usage (calculate cost from input + output tokens)
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens
        cost = (input_tokens / 1000) * PRICING["gpt-3.5-turbo-input"] + \
               (output_tokens / 1000) * PRICING["gpt-3.5-turbo-output"]
        log_api_usage("auto_tagging", "gpt-3.5-turbo", 
                      input_tokens=input_tokens, 
                      output_tokens=output_tokens, 
                      cost=cost)
        
        # Parse tags from response
        response_text = response.choices[0].message.content.strip()
        tags = json.loads(response_text)
        
        print(f"‚úÖ Extracted {len(tags)} tags: {tags[:5]}..." if len(tags) > 5 else f"‚úÖ Extracted {len(tags)} tags: {tags}")
        
        return tags
        
    except Exception as e:
        print(f"‚ö†Ô∏è Auto-tagging failed: {str(e)}")
        return []




# Create FastAPI app
app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize database on startup
@app.on_event("startup")
async def startup_event():
    """Initialize database tables on app startup"""
    print("üöÄ FastAPI startup event triggered")
    init_database()
    print("‚úÖ App initialization complete - ready to accept requests")

@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui():
    return get_swagger_ui_html(openapi_url="/openapi.json", title="Axis of Mind API Docs")


# Pydantic model for base64 upload
class Base64Upload(BaseModel):
    pdf_base64: str
    filename: str = "document.pdf"


# === Upload PDF (Base64 for ChatGPT) ===
@app.post("/upload-base64")
async def upload_pdf_base64(data: Base64Upload):
    try:
        # Log the raw incoming JSON structure
        print(f"üì• Raw JSON received: {data.model_dump()}")
        print(f"üõ¨ Received base64 file: {data.filename}")
        print(f"üìä Base64 length: {len(data.pdf_base64)} characters")
        
        # Fix base64 padding if needed
        base64_str = data.pdf_base64
        missing_padding = len(base64_str) % 4
        if missing_padding:
            base64_str += '=' * (4 - missing_padding)
            print(f"üîß Added {4 - missing_padding} padding characters")
        
        # Decode base64 to bytes
        pdf_bytes = base64.b64decode(base64_str)
        
        # Check for PDF EOF marker
        if not pdf_bytes.endswith(b'%%EOF'):
            print("‚ö†Ô∏è PDF may be corrupted or incomplete (EOF not found)")
        
        pdf_reader = PdfReader(io.BytesIO(pdf_bytes))
        page_count = len(pdf_reader.pages)
        text = " ".join(page.extract_text() or "" for page in pdf_reader.pages)
        chunks = chunk_text(text)
        print(f"üìÑ Uploading {len(chunks)} chunks to Pinecone")

        doc_id = str(uuid.uuid4())
        openai_client = get_openai_client()
        pinecone_index = get_pinecone_client()

        if not openai_client or not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "OpenAI or Pinecone client not initialized"})

        # Auto-tag document with InnerVerse taxonomy
        tags = await auto_tag_document(text, data.filename, openai_client)

        # Batch embedding + upsert
        vectors_to_upsert = []
        for i, chunk in enumerate(chunks):
            # Show progress for large documents
            if i % 50 == 0 and i > 0:
                print(f"üìä Processing chunk {i}/{len(chunks)}...")
            
            response = openai_client.embeddings.create(
                input=chunk, model="text-embedding-ada-002", timeout=60)
            vector = response.data[0].embedding
            vectors_to_upsert.append((f"{doc_id}-{i}", vector, {
                "text": chunk,
                "doc_id": doc_id,
                "filename": data.filename,
                "upload_timestamp": datetime.now().isoformat(),
                "tags": tags
            }))

        # Upload in batches to avoid Pinecone's 4MB request limit
        if vectors_to_upsert:
            batch_size = 50  # Safe batch size to stay under 4MB
            total_batches = (len(vectors_to_upsert) + batch_size - 1) // batch_size
            
            for batch_num in range(0, len(vectors_to_upsert), batch_size):
                batch = vectors_to_upsert[batch_num:batch_num + batch_size]
                pinecone_index.upsert(vectors=batch)
                current_batch = (batch_num // batch_size) + 1
                print(f"üì§ Uploaded batch {current_batch}/{total_batches} ({len(batch)} vectors)")
            
            print(f"‚úÖ Successfully uploaded {len(vectors_to_upsert)} total chunks")

        return {
            "message": "PDF uploaded and indexed with InnerVerse Intelligence Layer",
            "document_id": doc_id,
            "chunks_count": len(chunks),
            "tags": tags,
            "tags_count": len(tags)
        }

    except Exception as e:
        print(f"‚ùå Upload error: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Upload PDF (File for regular clients) ===
@app.post("/upload")
async def upload_pdf(file: UploadFile = File(...)):
    try:
        # ‚úÖ Debug log ‚Äî confirms if file is even received
        file_size_mb = 0
        print(f"üõ¨ Received file: {file.filename}")

        contents = await file.read()
        file_size_mb = len(contents) / (1024 * 1024)  # Convert to MB
        print(f"üì¶ File size: {file_size_mb:.2f} MB")
        
        pdf_reader = PdfReader(io.BytesIO(contents))
        page_count = len(pdf_reader.pages)
        text = " ".join(page.extract_text() or "" for page in pdf_reader.pages)
        chunks = chunk_text(text)
        print(f"üìÑ Processing {len(chunks)} chunks from {page_count} pages")

        doc_id = str(uuid.uuid4())
        openai_client = get_openai_client()
        pinecone_index = get_pinecone_client()

        if not openai_client or not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "OpenAI or Pinecone client not initialized"})

        # Auto-tag document with InnerVerse taxonomy
        tags = await auto_tag_document(text, file.filename, openai_client)

        # Batch embedding + upsert
        vectors_to_upsert = []
        embed_start = datetime.now()
        
        for i, chunk in enumerate(chunks):
            # Show progress for large documents
            if i % 50 == 0:
                elapsed = (datetime.now() - embed_start).total_seconds()
                print(f"üìä Processing chunk {i}/{len(chunks)} (elapsed: {elapsed:.1f}s)...")
            
            try:
                response = openai_client.embeddings.create(
                    input=chunk, model="text-embedding-ada-002", timeout=120)  # Increased timeout
                vector = response.data[0].embedding
                vectors_to_upsert.append((f"{doc_id}-{i}", vector, {
                    "text": chunk,
                    "doc_id": doc_id,
                    "filename": file.filename,
                    "upload_timestamp": datetime.now().isoformat(),
                    "tags": tags
                }))
            except Exception as embed_error:
                print(f"‚ùå Embedding error on chunk {i}: {str(embed_error)}")
                raise Exception(f"Failed to embed chunk {i}/{len(chunks)}: {str(embed_error)}")

        # Upload in batches to avoid Pinecone's 4MB request limit
        if vectors_to_upsert:
            batch_size = 50  # Safe batch size to stay under 4MB
            total_batches = (len(vectors_to_upsert) + batch_size - 1) // batch_size
            upsert_start = datetime.now()
            
            for batch_num in range(0, len(vectors_to_upsert), batch_size):
                batch = vectors_to_upsert[batch_num:batch_num + batch_size]
                try:
                    pinecone_index.upsert(vectors=batch)
                    current_batch = (batch_num // batch_size) + 1
                    print(f"üì§ Uploaded batch {current_batch}/{total_batches} ({len(batch)} vectors)")
                except Exception as upsert_error:
                    print(f"‚ùå Pinecone upsert error on batch {current_batch}: {str(upsert_error)}")
                    raise Exception(f"Failed to upload to Pinecone: {str(upsert_error)}")
            
            upsert_elapsed = (datetime.now() - upsert_start).total_seconds()
            total_elapsed = (datetime.now() - embed_start).total_seconds()
            print(f"‚úÖ Successfully uploaded {len(vectors_to_upsert)} chunks")
            print(f"‚è±Ô∏è Total time: {total_elapsed:.1f}s (upload: {upsert_elapsed:.1f}s)")

        return {
            "message": "PDF uploaded and indexed with InnerVerse Intelligence Layer",
            "document_id": doc_id,
            "chunks_count": len(chunks),
            "filename": file.filename,
            "tags": tags,
            "tags_count": len(tags)
        }

    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Upload error for {file.filename if 'file' in locals() else 'unknown'}: {error_msg}")
        # Return more detailed error
        return JSONResponse(status_code=500, content={
            "error": f"Upload failed: {error_msg}",
            "filename": file.filename if 'file' in locals() else "unknown"
        })


# === Delete ALL Documents ===
@app.delete("/documents/all")
async def delete_all_documents():
    """Delete ALL vectors from Pinecone index"""
    try:
        pinecone_index = get_pinecone_client()
        
        if not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "Pinecone client not initialized"})
        
        # Delete all vectors (delete_all is faster than filtering)
        pinecone_index.delete(delete_all=True)
        
        print(f"‚úÖ Deleted ALL vectors from Pinecone index")
        
        return {
            "message": "All documents deleted successfully"
        }
        
    except Exception as e:
        print(f"‚ùå Delete all error: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Delete Single Document ===
@app.delete("/documents/{document_id}")
async def delete_document(document_id: str):
    """Delete all vectors associated with a document ID"""
    try:
        pinecone_index = get_pinecone_client()
        
        if not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "Pinecone client not initialized"})
        
        # Delete all vectors with this doc_id using metadata filter
        pinecone_index.delete(filter={"doc_id": document_id})
        
        print(f"‚úÖ Deleted all vectors for document: {document_id}")
        
        return {
            "message": "Document deleted successfully",
            "document_id": document_id
        }
        
    except Exception as e:
        print(f"‚ùå Delete error: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Rename Document ===
class RenameDocumentRequest(BaseModel):
    new_filename: str

@app.patch("/documents/{document_id}/rename")
async def rename_document(document_id: str, request: RenameDocumentRequest):
    """Rename a document by updating its filename metadata in all vectors"""
    try:
        pinecone_index = get_pinecone_client()
        
        if not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "Pinecone client not initialized"})
        
        # First, fetch all vectors for this document
        dummy_vector = [0.0] * 1536
        query_response = pinecone_index.query(
            vector=dummy_vector,
            filter={"doc_id": document_id},
            top_k=10000,
            include_metadata=True
        )
        
        # Get matches
        try:
            matches = query_response.matches
        except AttributeError:
            matches = query_response.get("matches", [])
        
        if not matches:
            return JSONResponse(
                status_code=404,
                content={"error": "Document not found"})
        
        # Update each vector's metadata with new filename
        updates = []
        for match in matches:
            vector_id = getattr(match, "id", None) or match.get("id")
            metadata = getattr(match, "metadata", None) or match.get("metadata", {})
            
            # Update filename in metadata
            updated_metadata = dict(metadata) if hasattr(metadata, "items") else metadata
            updated_metadata["filename"] = request.new_filename
            
            updates.append({
                "id": vector_id,
                "metadata": updated_metadata
            })
        
        # Batch update in Pinecone (50 at a time)
        batch_size = 50
        for i in range(0, len(updates), batch_size):
            batch = updates[i:i + batch_size]
            
            # Use upsert with same IDs to update metadata
            upsert_data = []
            for item in batch:
                # Get the original vector values
                fetch_response = pinecone_index.fetch(ids=[item["id"]])
                if hasattr(fetch_response, "vectors"):
                    vectors_dict = fetch_response.vectors
                else:
                    vectors_dict = fetch_response.get("vectors", {})
                
                if item["id"] in vectors_dict:
                    vector_data = vectors_dict[item["id"]]
                    values = getattr(vector_data, "values", None) or vector_data.get("values", [])
                    
                    upsert_data.append((
                        item["id"],
                        values,
                        item["metadata"]
                    ))
            
            if upsert_data:
                pinecone_index.upsert(vectors=upsert_data)
        
        print(f"‚úÖ Renamed document {document_id} to '{request.new_filename}' ({len(updates)} vectors updated)")
        
        return {
            "message": "Document renamed successfully",
            "document_id": document_id,
            "new_filename": request.new_filename,
            "vectors_updated": len(updates)
        }
        
    except Exception as e:
        print(f"‚ùå Rename error: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Generate Document Report ===
@app.get("/documents/report")
async def get_documents_report():
    """Generate a CSV report of all uploaded documents"""
    try:
        pinecone_index = get_pinecone_client()
        
        if not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "Pinecone client not initialized"})
        
        # Query Pinecone to get documents
        # We'll use a dummy vector query with high top_k to get many results
        dummy_vector = [0.0] * 1536  # OpenAI ada-002 has 1536 dimensions
        
        query_response = pinecone_index.query(
            vector=dummy_vector,
            top_k=10000,  # Get up to 10k results
            include_metadata=True
        )
        
        # Extract unique documents with timestamps
        documents = {}
        try:
            matches = query_response.matches  # type: ignore
        except AttributeError:
            matches = query_response.get("matches", [])  # type: ignore
        
        for match in matches:
            metadata = getattr(match, "metadata", None)
            if not metadata:
                try:
                    metadata = match.get("metadata", {})
                except (AttributeError, TypeError):
                    metadata = {}
            
            if metadata:
                doc_id = metadata.get("doc_id") if hasattr(metadata, "get") else getattr(metadata, "doc_id", None)
                filename = metadata.get("filename", "Unknown") if hasattr(metadata, "get") else getattr(metadata, "filename", "Unknown")
                timestamp = metadata.get("upload_timestamp", "N/A") if hasattr(metadata, "get") else getattr(metadata, "upload_timestamp", "N/A")
                
                if doc_id and doc_id not in documents:
                    documents[doc_id] = {"filename": filename, "timestamp": timestamp}
        
        # Generate CSV
        output = io.StringIO()
        writer = csv.writer(output)
        writer.writerow(["document_id", "title", "uploaded_at"])
        
        # Sort documents chronologically (earliest to latest) by timestamp
        sorted_docs = sorted(
            documents.items(),
            key=lambda x: x[1]["timestamp"] if x[1]["timestamp"] != "N/A" else "9999-99-99"
        )
        
        # Hawaii timezone (HST = UTC-10)
        hawaii_tz = timezone(timedelta(hours=-10))
        
        for doc_id, doc_info in sorted_docs:
            # Format timestamp nicely if available
            timestamp_str = doc_info["timestamp"]
            if timestamp_str != "N/A":
                try:
                    # Convert ISO format to Hawaii time
                    dt = datetime.fromisoformat(timestamp_str)
                    # If no timezone info, assume UTC
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    # Convert to Hawaii time
                    dt_hawaii = dt.astimezone(hawaii_tz)
                    timestamp_str = dt_hawaii.strftime("%Y-%m-%d %I:%M %p")
                except:
                    pass  # Keep as-is if parsing fails
            
            writer.writerow([doc_id, doc_info["filename"], timestamp_str])
        
        # Return as downloadable file
        csv_content = output.getvalue()
        output.close()
        
        return StreamingResponse(
            io.StringIO(csv_content),
            media_type="text/csv",
            headers={
                "Content-Disposition": "attachment; filename=document_report.csv"
            }
        )
        
    except Exception as e:
        print(f"‚ùå Report generation error: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Get Tagged Documents (Cloud-based Tag Library) ===
@app.get("/api/tagged-documents")
async def get_tagged_documents():
    """Get all documents with their tags from Pinecone (cloud-based tag library)"""
    try:
        pinecone_index = get_pinecone_client()
        
        if not pinecone_index:
            return JSONResponse(
                status_code=500,
                content={"error": "Pinecone client not initialized"})
        
        # Query Pinecone to get all documents
        dummy_vector = [0.0] * 1536  # OpenAI ada-002 has 1536 dimensions
        
        query_response = pinecone_index.query(
            vector=dummy_vector,
            top_k=10000,  # Get up to 10k results
            include_metadata=True
        )
        
        # Extract unique documents with tags
        documents = {}
        try:
            matches = query_response.matches
        except AttributeError:
            matches = query_response.get("matches", [])
        
        for match in matches:
            metadata = getattr(match, "metadata", None)
            if not metadata:
                try:
                    metadata = match.get("metadata", {})
                except (AttributeError, TypeError):
                    metadata = {}
            
            if metadata:
                doc_id = metadata.get("doc_id") if hasattr(metadata, "get") else getattr(metadata, "doc_id", None)
                filename = metadata.get("filename", "Unknown") if hasattr(metadata, "get") else getattr(metadata, "filename", "Unknown")
                timestamp = metadata.get("upload_timestamp", None) if hasattr(metadata, "get") else getattr(metadata, "upload_timestamp", None)
                tags = metadata.get("tags", []) if hasattr(metadata, "get") else getattr(metadata, "tags", [])
                
                if doc_id and doc_id not in documents:
                    documents[doc_id] = {
                        "filename": filename,
                        "tags": tags if isinstance(tags, list) else [],
                        "timestamp": timestamp or datetime.now().isoformat()
                    }
        
        print(f"‚úÖ Retrieved {len(documents)} tagged documents from Pinecone")
        
        return {
            "documents": documents
        }
        
    except Exception as e:
        print(f"‚ùå Tagged documents retrieval error: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Query PDF for an answer ===
class QueryRequest(BaseModel):
    document_id: str = ""  # Optional - empty string means search all documents
    question: str
    tags: list = []  # Optional - filter by InnerVerse taxonomy tags

# === YouTube Transcription Request ===
class YouTubeTranscribeRequest(BaseModel):
    youtube_url: str

# === Text to PDF Request ===
class TextToPDFRequest(BaseModel):
    text: str
    title: str = "Document"

@app.post("/query")
async def query_pdf(request: QueryRequest, authorization: str = Header(None)):
    # API Key validation (only if Authorization header is provided)
    # Web app can call without auth, but external calls (custom GPT) must provide valid token
    if authorization:
        expected_api_key = os.getenv("API_KEY")
        if not expected_api_key:
            raise HTTPException(status_code=500, detail="Server configuration error: API key not set")
        
        # Support both "Bearer <token>" and direct token formats
        token = authorization.replace("Bearer ", "").strip()
        if token != expected_api_key:
            raise HTTPException(status_code=403, detail="Invalid API key")
    
    # Rate limiting check
    allowed, count = check_rate_limit(max_requests_per_hour=100)
    if not allowed:
        return JSONResponse(
            status_code=429,
            content={
                "error": f"Rate limit exceeded. Maximum 100 requests per hour. You've made {count} requests in the last hour. Please try again later."
            }
        )
    
    document_id = request.document_id
    question = request.question
    filter_tags = request.tags
    openai_client = get_openai_client()
    pinecone_index = get_pinecone_client()

    if not openai_client or not pinecone_index:
        return JSONResponse(
            status_code=500,
            content={"error": "OpenAI or Pinecone client not initialized"})

    try:
        embed_response = openai_client.embeddings.create(
            input=question, model="text-embedding-ada-002")
        question_vector = embed_response.data[0].embedding
        
        # Log embedding usage
        embed_tokens = embed_response.usage.total_tokens
        embed_cost = (embed_tokens / 1000) * PRICING["text-embedding-ada-002"]
        log_api_usage("query_embedding", "text-embedding-ada-002", input_tokens=embed_tokens, cost=embed_cost)

        # Build Pinecone filter based on document_id and tags
        filter_conditions = []
        if document_id and document_id.strip():
            filter_conditions.append({"doc_id": document_id})
        if filter_tags and len(filter_tags) > 0:
            # Filter for documents that contain ANY of the specified tags
            filter_conditions.append({"tags": {"$in": filter_tags}})
        
        # Build final filter
        if len(filter_conditions) == 0:
            # No filters - search all documents
            print(f"üîç Searching across ALL documents")
            query_response = pinecone_index.query(
                vector=question_vector,
                top_k=5,
                include_metadata=True
            )
        elif len(filter_conditions) == 1:
            # Single filter
            filter_str = f"doc_id={document_id}" if document_id else f"tags={filter_tags}"
            print(f"üîç Searching with filter: {filter_str}")
            query_response = pinecone_index.query(
                vector=question_vector,
                top_k=5,
                include_metadata=True,
                filter=filter_conditions[0]
            )
        else:
            # Multiple filters - use $and
            print(f"üîç Searching with filters: doc_id={document_id}, tags={filter_tags}")
            query_response = pinecone_index.query(
                vector=question_vector,
                top_k=5,
                include_metadata=True,
                filter={"$and": filter_conditions}
            )

        try:
            matches = query_response.matches  # type: ignore
        except AttributeError:
            matches = query_response.get("matches", [])  # type: ignore
        
        contexts = []
        source_docs = set()
        
        for m in matches:
            if "metadata" in m and "text" in m["metadata"]:
                contexts.append(m["metadata"]["text"])
                # Track which documents the answer came from
                if "filename" in m["metadata"]:
                    source_docs.add(m["metadata"]["filename"])
        
        if not contexts:
            return {"answer": "No relevant information found in your documents."}

        context_text = "\n\n".join(contexts)
        print(f"\nüîç Sending context from {len(source_docs)} document(s) to GPT\n")

        # Include source information in the prompt
        sources_text = f"\n\nSources: {', '.join(source_docs)}" if source_docs else ""
        
        prompt = f"Answer the question based on the context below.\n\nContext:\n{context_text}\n\nQuestion: {question}\nAnswer:"

        completion = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "system",
                "content": "You are a helpful assistant that answers questions based on uploaded documents. Provide clear, accurate answers based on the context provided."
            }, {
                "role": "user",
                "content": prompt
            }],
            timeout=15  # Prevents hanging
        )

        answer = completion.choices[0].message.content or "No answer generated."
        
        # Log chat completion usage
        if completion.usage:
            input_tokens = completion.usage.prompt_tokens
            output_tokens = completion.usage.completion_tokens
            chat_cost = (input_tokens / 1000) * PRICING["gpt-3.5-turbo-input"] + (output_tokens / 1000) * PRICING["gpt-3.5-turbo-output"]
            log_api_usage("chat_completion", "gpt-3.5-turbo", input_tokens=input_tokens, output_tokens=output_tokens, cost=chat_cost)
        
        # Add source information to answer if searching all docs
        if not document_id or not document_id.strip():
            if source_docs:
                answer += f"\n\nüìö Sources: {', '.join(sorted(source_docs))}"
        
        return {"answer": answer}

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


# === Google Drive Integration ===
async def get_gdrive_access_token():
    """Get Google Drive access token from Replit Connector"""
    try:
        hostname = os.getenv("REPLIT_CONNECTORS_HOSTNAME")
        x_replit_token = (
            f"repl {os.getenv('REPL_IDENTITY')}" if os.getenv('REPL_IDENTITY')
            else f"depl {os.getenv('WEB_REPL_RENEWAL')}" if os.getenv('WEB_REPL_RENEWAL')
            else None
        )
        
        if not x_replit_token or not hostname:
            return None
            
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"https://{hostname}/api/v2/connection?include_secrets=true&connector_names=google-drive",
                headers={
                    "Accept": "application/json",
                    "X_REPLIT_TOKEN": x_replit_token
                }
            )
            data = response.json()
            if data.get("items") and len(data["items"]) > 0:
                settings = data["items"][0].get("settings", {})
                return settings.get("access_token") or settings.get("oauth", {}).get("credentials", {}).get("access_token")
        return None
    except Exception as e:
        print(f"‚ùå Google Drive token error: {e}")
        return None

@app.get("/api/google-api-key")
async def get_google_api_key():
    """Return Google API key for Picker"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        return JSONResponse(status_code=500, content={"error": "Google API key not configured"})
    return {"api_key": api_key}

@app.get("/api/gdrive-token")
async def get_gdrive_token():
    """Get Google Drive OAuth access token"""
    token = await get_gdrive_access_token()
    if not token:
        return JSONResponse(status_code=401, content={"error": "Google Drive not connected"})
    return {"access_token": token}

@app.get("/api/gdrive-list-pdfs")
async def list_gdrive_pdfs():
    """List all PDF files from Google Drive"""
    try:
        token = await get_gdrive_access_token()
        if not token:
            return JSONResponse(status_code=401, content={"error": "Google Drive not connected"})
        
        async with httpx.AsyncClient() as client:
            # List PDF files from Google Drive
            response = await client.get(
                "https://www.googleapis.com/drive/v3/files",
                headers={"Authorization": f"Bearer {token}"},
                params={
                    "q": "mimeType='application/pdf' and trashed=false",
                    "fields": "files(id,name,size,modifiedTime)",
                    "orderBy": "modifiedTime desc",
                    "pageSize": 100
                }
            )
            
            if response.status_code != 200:
                return JSONResponse(status_code=response.status_code, content={"error": "Failed to list files"})
            
            data = response.json()
            return {"files": data.get("files", [])}
            
    except Exception as e:
        print(f"‚ùå Google Drive list error: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/api/gdrive-download/{file_id}")
async def download_gdrive_file(file_id: str):
    """Download a file from Google Drive and return as base64"""
    try:
        token = await get_gdrive_access_token()
        if not token:
            return JSONResponse(status_code=401, content={"error": "Google Drive not connected"})
        
        async with httpx.AsyncClient() as client:
            # Download file from Google Drive
            response = await client.get(
                f"https://www.googleapis.com/drive/v3/files/{file_id}?alt=media",
                headers={"Authorization": f"Bearer {token}"},
                timeout=30.0
            )
            
            if response.status_code != 200:
                return JSONResponse(status_code=response.status_code, content={"error": "Failed to download file"})
            
            # Convert to base64
            pdf_bytes = response.content
            pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')
            
            return {"pdf_base64": pdf_base64}
            
    except Exception as e:
        print(f"‚ùå Google Drive download error: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# === YouTube Transcription to PDF ===
@app.post("/transcribe-youtube")
async def transcribe_youtube(request: YouTubeTranscribeRequest):
    """Download YouTube audio, transcribe with Whisper, and generate PDF"""
    try:
        youtube_url = request.youtube_url
        print(f"üé¨ Processing YouTube URL: {youtube_url}")
        
        # Create temp directory for audio files
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "audio.mp3")
        
        # Step 1: Download audio using yt-dlp
        print("üì• Downloading audio from YouTube...")
        try:
            # Check if yt-dlp is available
            try:
                yt_dlp_check = subprocess.run(["yt-dlp", "--version"], capture_output=True, text=True, timeout=5)
                if yt_dlp_check.returncode != 0:
                    raise Exception("yt-dlp is not installed or not working")
            except FileNotFoundError:
                return JSONResponse(status_code=500, content={
                    "error": "System error: transcription service unavailable. Please try again later."
                })
            
            # Get video info first
            info_command = [
                "yt-dlp",
                "--print", "%(title)s|||%(duration)s",
                youtube_url
            ]
            info_result = subprocess.run(info_command, capture_output=True, text=True, timeout=30)
            
            if info_result.returncode != 0:
                # Parse error to provide helpful message
                error_msg = info_result.stderr.lower()
                
                if "private video" in error_msg or "members-only" in error_msg:
                    return JSONResponse(status_code=403, content={
                        "error": "This video is private or members-only. Try a different video."
                    })
                elif "age-restricted" in error_msg or "sign in to confirm your age" in error_msg:
                    return JSONResponse(status_code=403, content={
                        "error": "Age-restricted video. Your cookies may have expired. Update youtube_cookies.txt file."
                    })
                elif "video unavailable" in error_msg or "removed" in error_msg:
                    return JSONResponse(status_code=404, content={
                        "error": "Video unavailable or removed. Try a different video."
                    })
                elif "region" in error_msg or "not available in your country" in error_msg:
                    return JSONResponse(status_code=403, content={
                        "error": "Video blocked in this region. Try a different video."
                    })
                else:
                    return JSONResponse(status_code=400, content={
                        "error": "Unable to access video. If multiple videos fail, your YouTube cookies may be expired - update youtube_cookies.txt and republish. Otherwise, check the URL or try a different video."
                    })
            
            info_parts = info_result.stdout.strip().split("|||")
            video_title = info_parts[0] if len(info_parts) > 0 else "YouTube Video"
            video_duration = int(info_parts[1]) if len(info_parts) > 1 and info_parts[1].isdigit() else 0
            
            print(f"üì∫ Video: {video_title} ({video_duration}s / {video_duration//60} min)")
            
            # Try to find ffmpeg location
            ffmpeg_location = None
            try:
                ffmpeg_result = subprocess.run(["which", "ffmpeg"], capture_output=True, text=True, timeout=5)
                if ffmpeg_result.returncode == 0 and ffmpeg_result.stdout.strip():
                    ffmpeg_location = ffmpeg_result.stdout.strip()
                    print(f"‚úÖ Found ffmpeg at: {ffmpeg_location}")
                else:
                    print("‚ö†Ô∏è ffmpeg not found in PATH")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not check for ffmpeg: {e}")
            
            # Check for YouTube cookies file
            cookies_path = None
            cookies_file = "youtube_cookies.txt"
            
            if os.path.exists(cookies_file):
                # Read cookies from file
                with open(cookies_file, "r") as f:
                    cookies_content = f.read().strip()
                
                # Only use if file has actual cookies (not just comments)
                if cookies_content and not cookies_content.startswith("# Paste your"):
                    cookies_path = os.path.join(temp_dir, "cookies.txt")
                    with open(cookies_path, "w") as f:
                        f.write(cookies_content)
                    print(f"‚úÖ Using YouTube cookies from file for authentication")
                else:
                    print(f"‚ö†Ô∏è youtube_cookies.txt exists but is empty or not configured")
            else:
                print(f"‚ö†Ô∏è youtube_cookies.txt not found - some videos may be restricted")
            
            # Download audio with compression (32kbps mono for Whisper)
            # This keeps files under 25MB for videos up to ~90 minutes
            download_command = [
                "yt-dlp",
                "-x",  # Extract audio
                "--audio-format", "mp3",
                "--postprocessor-args", "ffmpeg:-ac 1 -ar 16000 -b:a 32k",  # Mono, 16kHz, 32kbps
                "-o", audio_path,
                youtube_url
            ]
            
            # Add cookies if available
            if cookies_path:
                download_command.insert(1, "--cookies")
                download_command.insert(2, cookies_path)
            
            # Add ffmpeg location if found
            if ffmpeg_location and os.path.dirname(ffmpeg_location):
                download_command.insert(1, "--ffmpeg-location")
                download_command.insert(2, os.path.dirname(ffmpeg_location))
            
            result = subprocess.run(download_command, capture_output=True, text=True, timeout=1800)
            
            if result.returncode != 0:
                # Parse download error
                error_msg = result.stderr.lower()
                
                if "http error 403" in error_msg or "forbidden" in error_msg:
                    return JSONResponse(status_code=403, content={
                        "error": "Video blocked. Your cookies may have expired. Update youtube_cookies.txt file."
                    })
                elif "http error 404" in error_msg:
                    return JSONResponse(status_code=404, content={
                        "error": "Video not found. Check the URL and try again."
                    })
                elif "sign in" in error_msg or "login" in error_msg:
                    return JSONResponse(status_code=401, content={
                        "error": "Login required. Add your cookies to youtube_cookies.txt file to access this video."
                    })
                else:
                    return JSONResponse(status_code=500, content={
                        "error": "Download failed. Try a different video or refresh your cookies."
                    })
            
            # Check if file was created
            if not os.path.exists(audio_path):
                return JSONResponse(status_code=500, content={
                    "error": "Audio extraction failed. Try a different video."
                })
                
            file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
            print(f"‚úÖ Audio downloaded: {file_size_mb:.1f}MB")
            
        except subprocess.TimeoutExpired:
            return JSONResponse(status_code=408, content={
                "error": "Download timed out after 30 minutes. This could be due to slow network, video restrictions, or expired cookies. Try refreshing your cookies or a different video."
            })
        except Exception as e:
            error_str = str(e).lower()
            if "cookie" in error_str:
                return JSONResponse(status_code=401, content={
                    "error": "Cookie error. Check your youtube_cookies.txt file."
                })
            else:
                return JSONResponse(status_code=500, content={
                    "error": f"Unable to process video. Try a different one."
                })
        
        # Step 2: Transcribe with OpenAI Whisper
        print("üé§ Transcribing with Whisper...")
        try:
            openai_client = get_openai_client()
            if not openai_client:
                return JSONResponse(status_code=500, content={"error": "OpenAI client not initialized"})
            
            file_size = os.path.getsize(audio_path)
            file_size_mb = file_size / (1024 * 1024)
            
            # If file is under 25MB, transcribe directly
            if file_size < 25 * 1024 * 1024:
                print(f"üìù File size {file_size_mb:.1f}MB - transcribing directly")
                
                # Get audio duration for cost calculation
                audio = AudioSegment.from_file(audio_path)
                duration_minutes = len(audio) / (1000 * 60)  # Convert ms to minutes
                
                with open(audio_path, "rb") as audio_file:
                    transcript_response = openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file,
                        response_format="text",
                        timeout=600  # 10 minute timeout for Whisper
                    )
                transcript = transcript_response if isinstance(transcript_response, str) else transcript_response.text
                
                # Log Whisper API usage
                whisper_cost = duration_minutes * PRICING["whisper-1"]
                log_api_usage("whisper", "whisper-1", input_tokens=0, output_tokens=0, cost=whisper_cost)
                print(f"üí∞ Whisper cost: ${whisper_cost:.4f} ({duration_minutes:.2f} minutes)")
                
            else:
                # File is too large - split into chunks
                print(f"üìù File size {file_size_mb:.1f}MB - splitting into chunks for transcription")
                
                # Load audio with pydub
                audio = AudioSegment.from_file(audio_path)
                chunk_length_ms = 10 * 60 * 1000  # 10 minutes per chunk
                total_chunks = len(audio) // chunk_length_ms + (1 if len(audio) % chunk_length_ms else 0)
                total_duration_minutes = len(audio) / (1000 * 60)
                
                transcriptions = []
                
                for i in range(0, len(audio), chunk_length_ms):
                    chunk = audio[i:i + chunk_length_ms]
                    chunk_path = os.path.join(temp_dir, f"chunk_{i//chunk_length_ms}.mp3")
                    chunk_duration_minutes = len(chunk) / (1000 * 60)
                    
                    # Export chunk as compressed MP3
                    chunk.export(chunk_path, format="mp3", bitrate="32k", parameters=["-ac", "1", "-ar", "16000"])
                    
                    # Transcribe chunk
                    chunk_num = i // chunk_length_ms + 1
                    print(f"  üìù Transcribing chunk {chunk_num}/{total_chunks}...")
                    
                    with open(chunk_path, "rb") as chunk_file:
                        chunk_response = openai_client.audio.transcriptions.create(
                            model="whisper-1",
                            file=chunk_file,
                            response_format="text",
                            timeout=600  # 10 minute timeout per chunk
                        )
                    
                    chunk_transcript = chunk_response if isinstance(chunk_response, str) else chunk_response.text
                    transcriptions.append(chunk_transcript)
                    
                    # Log Whisper API usage for this chunk
                    chunk_cost = chunk_duration_minutes * PRICING["whisper-1"]
                    log_api_usage("whisper", "whisper-1", input_tokens=0, output_tokens=0, cost=chunk_cost)
                    
                    # Clean up chunk file
                    os.remove(chunk_path)
                
                # Combine all transcriptions
                transcript = " ".join(transcriptions)
                print(f"‚úÖ Combined {len(transcriptions)} chunks")
                print(f"üí∞ Total Whisper cost: ${total_duration_minutes * PRICING['whisper-1']:.4f} ({total_duration_minutes:.2f} minutes)")
            
            print(f"‚úÖ Transcription complete: {len(transcript)} characters")
            
        except Exception as e:
            error_str = str(e).lower()
            
            if "quota" in error_str or "rate limit" in error_str:
                return JSONResponse(status_code=429, content={
                    "error": "OpenAI rate limit reached. Wait a few minutes and try again."
                })
            elif "api key" in error_str or "unauthorized" in error_str:
                return JSONResponse(status_code=401, content={
                    "error": "OpenAI API key error. Check your OPENAI_API_KEY in Secrets."
                })
            elif "timeout" in error_str:
                return JSONResponse(status_code=408, content={
                    "error": "Transcription timed out. Try a shorter video."
                })
            else:
                return JSONResponse(status_code=500, content={
                    "error": "Transcription failed. The audio may be corrupted or too complex."
                })
        finally:
            # Clean up audio file
            try:
                if os.path.exists(audio_path):
                    os.remove(audio_path)
                if os.path.exists(temp_dir):
                    os.rmdir(temp_dir)
            except:
                pass
        
        # Step 3: Generate PDF
        print("üìÑ Generating PDF...")
        try:
            pdf_filename = f"transcript_{uuid.uuid4().hex[:8]}.pdf"
            pdf_path = os.path.join("/tmp", pdf_filename)
            
            # Create PDF
            doc = SimpleDocTemplate(pdf_path, pagesize=letter)
            styles = getSampleStyleSheet()
            
            # Custom styles
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=18,
                textColor='#5B21B6',
                spaceAfter=12
            )
            
            body_style = ParagraphStyle(
                'CustomBody',
                parent=styles['BodyText'],
                fontSize=11,
                leading=16,
                alignment=TA_LEFT
            )
            
            # Build PDF content
            story = []
            
            # Title
            story.append(Paragraph(f"<b>{video_title}</b>", title_style))
            story.append(Spacer(1, 0.2*inch))
            
            # Metadata
            metadata_style = ParagraphStyle('Metadata', parent=styles['Normal'], fontSize=9, textColor='gray')
            story.append(Paragraph(f"Transcribed on {datetime.now().strftime('%B %d, %Y at %I:%M %p')}", metadata_style))
            story.append(Paragraph(f"Source: {youtube_url}", metadata_style))
            story.append(Spacer(1, 0.3*inch))
            
            # Transcript text - split into paragraphs
            paragraphs = transcript.split('\n')
            for para in paragraphs:
                if para.strip():
                    story.append(Paragraph(para.strip(), body_style))
                    story.append(Spacer(1, 0.15*inch))
            
            # Build PDF
            doc.build(story)
            print(f"‚úÖ PDF created: {pdf_path}")
            
            # Return the PDF as a download
            return FileResponse(
                pdf_path,
                media_type="application/pdf",
                filename=f"{video_title[:50].replace('/', '-')}.pdf",
                headers={
                    "Content-Disposition": f"attachment; filename=\"{video_title[:50].replace('/', '-')}.pdf\""
                }
            )
            
        except Exception as e:
            return JSONResponse(status_code=500, content={
                "error": "Failed to create PDF. The transcription may contain special characters."
            })
        
    except Exception as e:
        print(f"‚ùå YouTube transcription error: {str(e)}")
        return JSONResponse(status_code=500, content={
            "error": "Something went wrong. Please try again or use a different video."
        })


# === FREE YouTube Transcript (No Whisper Cost) ===
@app.post("/transcribe-youtube-free")
async def transcribe_youtube_free(request: YouTubeTranscribeRequest):
    """Get FREE YouTube transcript (no Whisper API costs) - only works for videos with captions"""
    try:
        youtube_url = request.youtube_url
        print(f"üé¨ Getting FREE transcript for: {youtube_url}")
        
        # Extract video ID from URL
        video_id = None
        patterns = [
            r'(?:youtube\.com/watch\?v=|youtu\.be/)([^&\n?#]+)',
            r'youtube\.com/embed/([^&\n?#]+)',
            r'youtube\.com/v/([^&\n?#]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, youtube_url)
            if match:
                video_id = match.group(1)
                break
        
        if not video_id:
            return JSONResponse(status_code=400, content={
                "error": "Invalid YouTube URL. Please check the URL and try again."
            })
        
        print(f"üì∫ Video ID: {video_id}")
        
        # Try to get the transcript
        try:
            # Get transcript (tries manual first, then auto-generated, any language)
            # Use cookies to bypass YouTube's IP blocking for cloud providers
            session = get_youtube_session_with_cookies()
            ytt_api = YouTubeTranscriptApi(http_client=session)
            transcript_list = ytt_api.list(video_id)
            
            transcript_data = None
            language = 'unknown'
            
            # Try manual transcripts first (any language)
            try:
                for transcript in transcript_list:
                    if not transcript.is_generated:
                        transcript_data = transcript.fetch()
                        language = transcript.language
                        print(f"üìù Found manual transcript in {language}")
                        break
            except:
                pass
            
            # If no manual transcript, try auto-generated (any language)
            if not transcript_data:
                try:
                    for transcript in transcript_list:
                        if transcript.is_generated:
                            transcript_data = transcript.fetch()
                            language = f"{transcript.language} (auto)"
                            print(f"ü§ñ Found auto-generated transcript in {language}")
                            break
                except:
                    pass
            
            # If still nothing, try the simple fetch method as fallback
            if not transcript_data:
                transcript_data = ytt_api.fetch(video_id, languages=['en', 'es', 'fr', 'de', 'it', 'pt', 'ja', 'ko', 'zh', 'ru', 'ar', 'hi'])
                language = 'fallback'
            
            # Combine all transcript segments into one text
            transcript = ' '.join([entry['text'] for entry in transcript_data])
            
            print(f"‚úÖ FREE transcript retrieved ({len(transcript)} characters, {language})")
            
        except Exception as e:
            error_str = str(e).lower()
            print(f"‚ùå Transcript error: {e}")
            
            if 'transcript' in error_str and 'disabled' in error_str:
                return JSONResponse(status_code=404, content={
                    "error": "This video has captions disabled. Use the 'Transcribe with Whisper' button instead (costs ~$0.006/min)."
                })
            elif 'subtitles are disabled' in error_str or 'no transcript' in error_str:
                return JSONResponse(status_code=404, content={
                    "error": "No captions available for this video. Use the 'Transcribe with Whisper' button instead (costs ~$0.006/min)."
                })
            elif 'private' in error_str or 'unavailable' in error_str:
                return JSONResponse(status_code=403, content={
                    "error": "Video is private or unavailable. Try a different video."
                })
            else:
                return JSONResponse(status_code=404, content={
                    "error": f"Could not get captions for this video. Use the 'Transcribe with Whisper' button instead (costs ~$0.006/min)."
                })
        
        # Get video title using yt-dlp
        video_title = "YouTube Video"
        try:
            info_command = ["yt-dlp", "--print", "%(title)s", youtube_url]
            info_result = subprocess.run(info_command, capture_output=True, text=True, timeout=10)
            if info_result.returncode == 0 and info_result.stdout.strip():
                video_title = info_result.stdout.strip()
        except:
            pass  # Use default title if yt-dlp fails
        
        # Generate PDF
        print("üìÑ Generating FREE PDF...")
        try:
            pdf_filename = f"transcript_free_{uuid.uuid4().hex[:8]}.pdf"
            pdf_path = os.path.join("/tmp", pdf_filename)
            
            # Create PDF
            doc = SimpleDocTemplate(pdf_path, pagesize=letter)
            styles = getSampleStyleSheet()
            
            # Custom styles
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=18,
                textColor='#5B21B6',
                spaceAfter=12
            )
            
            body_style = ParagraphStyle(
                'CustomBody',
                parent=styles['BodyText'],
                fontSize=11,
                leading=16,
                alignment=TA_LEFT
            )
            
            # Build PDF content
            story = []
            
            # Title
            story.append(Paragraph(f"<b>{video_title}</b>", title_style))
            story.append(Spacer(1, 0.2*inch))
            
            # Metadata
            metadata_style = ParagraphStyle('Metadata', parent=styles['Normal'], fontSize=9, textColor='gray')
            story.append(Paragraph(f"FREE Transcript - {datetime.now().strftime('%B %d, %Y at %I:%M %p')}", metadata_style))
            story.append(Paragraph(f"Source: {youtube_url}", metadata_style))
            story.append(Paragraph(f"Language: {language}", metadata_style))
            story.append(Spacer(1, 0.3*inch))
            
            # Transcript text - split into paragraphs
            paragraphs = transcript.split('. ')  # Split on sentences for better formatting
            current_para = ""
            for sentence in paragraphs:
                current_para += sentence + ". "
                # Create a new paragraph every ~500 characters
                if len(current_para) > 500:
                    story.append(Paragraph(current_para.strip(), body_style))
                    story.append(Spacer(1, 0.15*inch))
                    current_para = ""
            
            # Add any remaining text
            if current_para.strip():
                story.append(Paragraph(current_para.strip(), body_style))
            
            # Build PDF
            doc.build(story)
            print(f"‚úÖ FREE PDF created: {pdf_path}")
            
            # Return the PDF as a download
            return FileResponse(
                pdf_path,
                media_type="application/pdf",
                filename=f"{video_title[:50].replace('/', '-')}_FREE_transcript.pdf",
                headers={
                    "Content-Disposition": f"attachment; filename=\"{video_title[:50].replace('/', '-')}_FREE_transcript.pdf\""
                }
            )
            
        except Exception as e:
            return JSONResponse(status_code=500, content={
                "error": "Failed to create PDF from transcript."
            })
        
    except Exception as e:
        print(f"‚ùå FREE YouTube transcript error: {str(e)}")
        return JSONResponse(status_code=500, content={
            "error": "Something went wrong. Please try again or use the Whisper transcription."
        })


# === Text to PDF with Punctuation Fixing ===
@app.post("/text-to-pdf")
async def text_to_pdf(request: TextToPDFRequest):
    """Convert text to PDF with AI-powered punctuation and grammar fixes"""
    try:
        openai_client = get_openai_client()
        if not openai_client:
            return JSONResponse(
                status_code=500,
                content={"error": "OpenAI client not initialized"})
        
        print(f"üìù Processing text for PDF generation...")
        
        # Use GPT to fix punctuation and grammar
        print("üîß Fixing punctuation and grammar with AI...")
        try:
            completion = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a professional editor. Fix all punctuation, grammar, and formatting errors in the text. Preserve the original meaning and tone. Add proper paragraph breaks where appropriate. Return only the corrected text, no explanations or comments."
                    },
                    {
                        "role": "user",
                        "content": request.text
                    }
                ],
                temperature=0.3
            )
            
            content = completion.choices[0].message.content
            cleaned_text = content.strip() if content else request.text
            print("‚úÖ Text cleaned and formatted")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Punctuation fix failed: {str(e)}, using original text")
            cleaned_text = request.text
        
        # Generate PDF
        print("üìÑ Generating PDF...")
        try:
            pdf_filename = f"document_{uuid.uuid4().hex[:8]}.pdf"
            pdf_path = os.path.join("/tmp", pdf_filename)
            
            # Create PDF
            doc = SimpleDocTemplate(pdf_path, pagesize=letter)
            styles = getSampleStyleSheet()
            
            # Custom styles
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=18,
                textColor='#5B21B6',
                spaceAfter=12
            )
            
            body_style = ParagraphStyle(
                'CustomBody',
                parent=styles['BodyText'],
                fontSize=11,
                leading=16,
                alignment=TA_LEFT
            )
            
            # Build PDF content
            story = []
            
            # Title
            story.append(Paragraph(f"<b>{request.title}</b>", title_style))
            story.append(Spacer(1, 0.2*inch))
            
            # Metadata
            metadata_style = ParagraphStyle('Metadata', parent=styles['Normal'], fontSize=9, textColor='gray')
            story.append(Paragraph(f"Created on {datetime.now().strftime('%B %d, %Y at %I:%M %p')}", metadata_style))
            story.append(Spacer(1, 0.3*inch))
            
            # Text content - split into paragraphs
            paragraphs = cleaned_text.split('\n')
            for para in paragraphs:
                if para.strip():
                    story.append(Paragraph(para.strip(), body_style))
                    story.append(Spacer(1, 0.15*inch))
            
            # Build PDF
            doc.build(story)
            print(f"‚úÖ PDF created: {pdf_path}")
            
            # Return the PDF as a download
            safe_filename = request.title[:50].replace('/', '-').replace('\\', '-')
            return FileResponse(
                pdf_path,
                media_type="application/pdf",
                filename=f"{safe_filename}.pdf",
                headers={
                    "Content-Disposition": f"attachment; filename=\"{safe_filename}.pdf\""
                }
            )
            
        except Exception as e:
            print(f"‚ùå PDF generation error: {str(e)}")
            return JSONResponse(status_code=500, content={
                "error": "Failed to create PDF. Please try again with different text."
            })
        
    except Exception as e:
        print(f"‚ùå Text to PDF error: {str(e)}")
        return JSONResponse(status_code=500, content={
            "error": "Something went wrong. Please try again."
        })


# === API Usage Endpoint ===
@app.get("/api/usage")
async def get_usage_stats():
    """Return API usage statistics for cost tracker"""
    try:
        conn = get_db_connection()
        if not conn:
            return {
                "total_cost": 0.0,
                "last_24h_cost": 0.0,
                "by_operation": {},
                "recent_calls": []
            }
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # Get total cost
        cursor.execute("SELECT COALESCE(SUM(cost), 0) as total_cost FROM api_usage")
        result = cursor.fetchone()
        total_cost = float(result["total_cost"]) if result else 0.0
        
        # Get last 24h cost (convert to Hawaii timezone for display)
        cursor.execute("""
            SELECT COALESCE(SUM(cost), 0) as last_24h_cost 
            FROM api_usage 
            WHERE timestamp >= NOW() - INTERVAL '24 hours'
        """)
        result = cursor.fetchone()
        last_24h_cost = float(result["last_24h_cost"]) if result else 0.0
        
        # Group by operation
        cursor.execute("""
            SELECT operation, 
                   COALESCE(SUM(cost), 0) as cost, 
                   COUNT(*) as count
            FROM api_usage
            GROUP BY operation
        """)
        by_operation = {}
        for row in cursor.fetchall():
            by_operation[row["operation"]] = {
                "cost": float(row["cost"]),
                "count": row["count"]
            }
        
        # Get recent 10 calls (convert to Hawaii timezone)
        cursor.execute("""
            SELECT operation, 
                   cost, 
                   timestamp
            FROM api_usage
            ORDER BY timestamp DESC
            LIMIT 10
        """)
        recent_calls = []
        hawaii_tz = pytz.timezone('Pacific/Honolulu')
        for row in cursor.fetchall():
            # Convert UTC timestamp to Hawaii timezone
            utc_time = row["timestamp"]
            if utc_time.tzinfo is None:
                utc_time = pytz.utc.localize(utc_time)
            hawaii_time = utc_time.astimezone(hawaii_tz)
            
            recent_calls.append({
                "operation": row["operation"],
                "cost": float(row["cost"]),
                "timestamp": hawaii_time.isoformat()
            })
        
        cursor.close()
        conn.close()
        
        return {
            "total_cost": round(total_cost, 6),
            "last_24h_cost": round(last_24h_cost, 6),
            "by_operation": by_operation,
            "recent_calls": recent_calls
        }
    except Exception as e:
        print(f"‚ùå Error calculating usage stats: {str(e)}")
        return {
            "total_cost": 0.0,
            "last_24h_cost": 0.0,
            "by_operation": {},
            "recent_calls": []
        }


# === Serve Frontend ===
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse

@app.get("/health", include_in_schema=False)
def health_check():
    """Health check endpoint for deployments"""
    return {"status": "healthy", "app": "InnerVerse"}

@app.get("/", include_in_schema=False)
def serve_frontend():
    return FileResponse("index.html", headers={"Cache-Control": "no-cache, no-store, must-revalidate"})

@app.get("/privacy", include_in_schema=False)
def serve_privacy():
    return FileResponse("privacy.html", headers={"Cache-Control": "no-cache, no-store, must-revalidate"})

# Mount static files (CSS, JS)
app.mount("/static", StaticFiles(directory="."), name="static")


# === Run the app ===
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 5000))
    print(f"üåê Starting server on 0.0.0.0:{port}")
    uvicorn.run(app, host="0.0.0.0", port=port)
