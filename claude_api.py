import os
import anthropic
from typing import List, Dict, Any
import psycopg2
from psycopg2.extras import RealDictCursor
from pinecone import Pinecone
import openai
import time
import json
from src.services.pinecone_organizer import extract_all_metadata, organize_results_by_metadata, format_organized_context
from src.services.conversation_context import get_or_create_context
from src.services.prompt_builder import build_system_prompt, PromptAssemblyError
from src.services.type_injection import get_type_stack

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")
API_KEY = os.getenv("API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = os.getenv("PINECONE_INDEX")
BRAVE_API_KEY = os.getenv("BRAVE_API_KEY")

# Load MBTI reference data at module level (loaded once on startup)
try:
    with open('src/data/reference_data.json', 'r') as f:
        REFERENCE_DATA = json.load(f)
    print("‚úÖ [REFERENCE DATA] Loaded MBTI reference data successfully")
except FileNotFoundError:
    print("‚ö†Ô∏è [REFERENCE DATA] reference_data.json not found - using Pinecone only")
    REFERENCE_DATA = {}
except Exception as e:
    print(f"‚ö†Ô∏è [REFERENCE DATA] Error loading reference_data.json: {e}")
    REFERENCE_DATA = {}

PROJECTS = [
    {"id": "relationship-lab", "name": "üíï Relationship Lab", "emoji": "üíï", "description": "Deep focus on golden pairs, compatibility, relationship dynamics"},
    {"id": "mbti-academy", "name": "üéì MBTI Academy", "emoji": "üéì", "description": "Structured learning on cognitive functions and type theory"},
    {"id": "type-detective", "name": "üîç Type Detective", "emoji": "üîç", "description": "Practice typing real people and analyzing behavior patterns"},
    {"id": "trading-psychology", "name": "üìà Trading Psychology", "emoji": "üìà", "description": "Analyze trading behavior through MBTI lens"},
    {"id": "pm-playbook", "name": "üíº PM Playbook", "emoji": "üíº", "description": "Product management applications of MBTI"},
    {"id": "quick-hits", "name": "‚ö° Quick Hits", "emoji": "‚ö°", "description": "Random questions that don't fit elsewhere"},
    {"id": "brain-dump", "name": "üß† Brain Dump", "emoji": "üß†", "description": "Capture thoughts and ideas"}
]

def get_db_connection():
    """Get PostgreSQL database connection"""
    if not DATABASE_URL:
        return None
    return psycopg2.connect(DATABASE_URL)

def get_pinecone_index():
    """Get Pinecone index client"""
    if not PINECONE_API_KEY or not PINECONE_INDEX:
        return None
    pc = Pinecone(api_key=PINECONE_API_KEY)
    return pc.Index(PINECONE_INDEX)

def extract_follow_up_question(text: str) -> str:
    """
    Extract follow-up question from Claude's response.
    Pattern: [FOLLOW-UP: question?]
    Returns: The extracted question string, or None if not found
    """
    import re
    follow_up_match = re.search(r'\[FOLLOW-UP:\s*(.+?)\]', text, re.IGNORECASE)
    if follow_up_match:
        return follow_up_match.group(1).strip()
    return None

def detect_functions_in_message(text: str) -> List[str]:
    """
    Detect cognitive functions mentioned in message.
    Returns list of function codes (e.g., ['Ni', 'Te', 'Fi'])
    """
    import re
    
    if not text:
        return []
    
    text_upper = text.upper()
    functions = ['NI', 'NE', 'SI', 'SE', 'TI', 'TE', 'FI', 'FE']
    detected = []
    
    for func in functions:
        # Match function code as standalone word or with common suffixes
        patterns = [
            rf'\b{func}\b',  # Standalone: "Ni hero"
            rf'\b{func} HERO\b',
            rf'\b{func} PARENT\b',
            rf'\b{func} CHILD\b',
            rf'\b{func} INFERIOR\b',
            rf'\b{func} NEMESIS\b',
            rf'\b{func} CRITIC\b',
            rf'\b{func} TRICKSTER\b',
            rf'\b{func} DEMON\b',
        ]
        
        if any(re.search(pattern, text_upper) for pattern in patterns):
            detected.append(func.capitalize())
    
    return list(set(detected))  # Remove duplicates

def rerank_chunks_with_metadata(chunks: List[Dict], user_question: str) -> List[Dict]:
    """
    Re-rank chunks using BOTH similarity score AND metadata relevance.
    Boosts chunks that match detected types, functions, and query intent.
    """
    from src.services.type_injection import detect_types_in_message
    
    detected_types = detect_types_in_message(user_question)
    detected_functions = detect_functions_in_message(user_question)
    question_lower = user_question.lower()
    
    for chunk in chunks:
        base_score = chunk.get('score', 0.0)
        boost = 0.0
        
        # Boost if types match (strongest signal)
        chunk_types = chunk.get('types_discussed', [])
        if chunk_types and detected_types:
            matching_types = set(chunk_types) & set(detected_types)
            if matching_types:
                boost += 0.12 * len(matching_types)  # Up to +0.24 for 2 types
        
        # Boost if functions match
        chunk_functions = chunk.get('functions_covered', [])
        if chunk_functions and detected_functions:
            matching_funcs = set(chunk_functions) & set(detected_functions)
            if matching_funcs:
                boost += 0.08 * len(matching_funcs)
        
        # Boost recent seasons (Season 20+ reflects latest thinking)
        season_str = chunk.get('season', '')
        if season_str:
            try:
                season_num = int(season_str)
                if season_num >= 20:
                    boost += 0.06
                elif season_num >= 15:
                    boost += 0.03
            except (ValueError, TypeError):
                pass
        
        # Boost if content_type matches query intent
        content_type = chunk.get('content_type', '').lower()
        
        # Relationship queries
        if any(keyword in question_lower for keyword in ['relationship', 'compatible', 'interact', 'pair', 'together']):
            if 'relationship' in content_type:
                boost += 0.10
        
        # Octagram queries
        if any(keyword in question_lower for keyword in ['octagram', 'udsf', 'uduf', 'sdsf', 'sduf', 'developed', 'focused']):
            if 'octagram' in content_type or 'development' in content_type:
                boost += 0.15
        
        # Function-specific queries
        if any(keyword in question_lower for keyword in ['function', 'hero', 'parent', 'child', 'inferior', 'shadow']):
            if 'function' in content_type or 'cognitive' in content_type:
                boost += 0.08
        
        # Type comparison queries
        if len(detected_types) >= 2:
            if 'comparison' in content_type or 'dynamics' in content_type:
                boost += 0.10
        
        # Apply boost (cap at 1.0)
        chunk['boosted_score'] = min(1.0, base_score + boost)
        chunk['boost_applied'] = boost
    
    # Re-sort by boosted score
    chunks.sort(key=lambda x: x.get('boosted_score', x.get('score', 0.0)), reverse=True)
    
    return chunks

def format_rag_context_professional(sorted_chunks: List[Dict]) -> str:
    """
    Format RAG chunks with metadata, citations, and confidence indicators.
    Makes it CRYSTAL CLEAR to Claude what's authoritative vs. uncertain.
    
    This structured format helps Claude:
    1. Prioritize high-confidence results
    2. Cite sources accurately
    3. Cross-reference multiple excerpts
    4. Flag when content is moderate confidence
    """
    if not sorted_chunks:
        return "No relevant content found in the knowledge base."
    
    context_parts = []
    context_parts.append("=" * 80)
    context_parts.append("üìö KNOWLEDGE BASE RESULTS")
    context_parts.append(f"Retrieved {len(sorted_chunks)} relevant excerpts from CS Joseph transcripts")
    context_parts.append("=" * 80)
    context_parts.append("")
    
    for i, chunk in enumerate(sorted_chunks, 1):
        score = chunk.get('boosted_score', chunk.get('score', 0.0))
        
        # Confidence tiers
        if score >= 0.85:
            confidence = "HIGH"
            emoji = "üü¢"
        elif score >= 0.75:
            confidence = "MEDIUM"
            emoji = "üü°"
        else:
            confidence = "MODERATE"
            emoji = "üü†"
        
        # Extract enriched metadata
        filename = chunk.get('filename', 'Unknown')
        content_type = chunk.get('content_type', '')
        season = chunk.get('season', '')
        types_discussed = chunk.get('types_discussed', [])
        functions = chunk.get('functions_covered', [])
        difficulty = chunk.get('difficulty', '')
        
        # Build metadata line
        metadata_parts = []
        if season:
            metadata_parts.append(f"Season {season}")
        if content_type:
            metadata_parts.append(content_type.replace('_', ' ').title())
        if types_discussed:
            metadata_parts.append(f"Types: {', '.join(types_discussed[:4])}")
        if not metadata_parts:
            # Fallback to filename if no metadata
            metadata_parts.append(filename)
        
        metadata_str = " | ".join(metadata_parts)
        
        # Format chunk with structure
        context_parts.append(f"{emoji} [EXCERPT #{i} - {confidence} CONFIDENCE]")
        context_parts.append(f"Score: {score:.3f}")
        context_parts.append(f"Source: {metadata_str}")
        
        if functions:
            context_parts.append(f"Functions Covered: {', '.join(functions[:6])}")
        
        if difficulty:
            context_parts.append(f"Difficulty: {difficulty.capitalize()}")
        
        context_parts.append("-" * 80)
        context_parts.append(chunk['text'])
        context_parts.append("")
    
    context_parts.append("=" * 80)
    context_parts.append("END OF KNOWLEDGE BASE RESULTS")
    context_parts.append(f"Total Excerpts: {len(sorted_chunks)}")
    context_parts.append("=" * 80)
    
    return "\n".join(context_parts)

def calculate_confidence_score(chunks: list, query: str) -> dict:
    """
    Calculate answer confidence based on retrieval quality.
    
    Args:
        chunks: Retrieved Pinecone chunks with scores
        query: User's query
        
    Returns:
        Dict with confidence level, score, and reasoning
    """
    if not chunks:
        return {
            "level": "none",
            "score": 0.0,
            "reasoning": "No relevant sources found",
            "stars": "‚≠ê"
        }
    
    # Average similarity score
    avg_score = sum(c.get('score', 0.0) for c in chunks) / len(chunks)
    
    # Number of high-quality chunks (score > 0.85)
    high_quality_count = sum(1 for c in chunks if c.get('score', 0.0) > 0.85)
    
    # Determine confidence level
    if high_quality_count >= 5 and avg_score > 0.88:
        level = "very_high"
        stars = "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"
        reasoning = f"{len(chunks)} highly relevant sources"
    elif high_quality_count >= 3 and avg_score > 0.85:
        level = "high"
        stars = "‚≠ê‚≠ê‚≠ê‚≠ê"
        reasoning = f"{high_quality_count} strong matches"
    elif len(chunks) >= 3 and avg_score > 0.80:
        level = "medium"
        stars = "‚≠ê‚≠ê‚≠ê"
        reasoning = f"{len(chunks)} moderate matches"
    elif len(chunks) >= 2:
        level = "low"
        stars = "‚≠ê‚≠ê"
        reasoning = f"Limited information ({len(chunks)} sources)"
    else:
        level = "very_low"
        stars = "‚≠ê"
        reasoning = "Insufficient information"
    
    return {
        "level": level,
        "score": avg_score,
        "reasoning": reasoning,
        "stars": stars,
        "source_count": len(chunks)
    }


def format_citations(chunks: list) -> str:
    """
    Format citations from retrieved chunks.
    
    Args:
        chunks: Retrieved Pinecone chunks
        
    Returns:
        Formatted citation string
    """
    citations = []
    
    for i, chunk in enumerate(chunks[:5], 1):  # Top 5 sources
        metadata = chunk.get('metadata', {}) if isinstance(chunk.get('metadata'), dict) else {}
        filename = metadata.get('filename', chunk.get('filename', 'Unknown'))
        season = metadata.get('season', chunk.get('season', 'Unknown'))
        match_score = chunk.get('score', chunk.get('boosted_score', 0.0))
        
        # Clean filename
        filename = filename.replace('.pdf', '').replace('_', ' ')
        
        citations.append(
            f"{i}. **Season {season}:** {filename} (Match: {match_score:.2f})"
        )
    
    return "\n".join(citations) if citations else "No sources available"


def extract_filters_from_query(query: str) -> dict:
    """
    Extract Pinecone filters from user query using GPT-4o-mini.
    
    Args:
        query: User's question
        
    Returns:
        Dict of Pinecone filters (empty dict if no filters extracted)
    """
    if not OPENAI_API_KEY:
        return {}
    
    try:
        openai.api_key = OPENAI_API_KEY
        
        prompt = f"""Analyze this user query and extract metadata filters for a vector database search.

Available metadata fields:
- season: ["1", "2", "3", "21", "22", etc.]
- types_discussed: ["INTJ", "ENFP", "INFJ", etc.]
- difficulty: ["foundation", "intermediate", "advanced", "expert"]
- primary_category: ["cognitive_functions", "type_profiles", "relationships", etc.]
- content_type: ["main_season", "csj_responds", "special", etc.]

User Query: "{query}"

Extract filters as JSON. Only include filters explicitly mentioned or strongly implied.

Examples:
- "According to Season 1..." ‚Üí {{"season": {{"$eq": "1"}}}}
- "How does ENFP develop?" ‚Üí {{"types_discussed": {{"$in": ["ENFP"]}}}}
- "Beginner guide to..." ‚Üí {{"difficulty": {{"$in": ["foundation", "intermediate"]}}}}

Your response (JSON only):"""
        
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Extract metadata filters from queries. Return valid JSON only."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=200
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Try to extract JSON if wrapped in markdown
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        filters = json.loads(response_text)
        
        # Validate filter structure
        if isinstance(filters, dict) and filters:
            print(f"üéØ [METADATA-FILTER] Extracted filters: {filters}")
            return filters
        else:
            return {}
            
    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è [METADATA-FILTER] JSON parsing failed: {e}")
        return {}
    except Exception as e:
        print(f"‚ö†Ô∏è [METADATA-FILTER] Filter extraction failed: {e}")
        return {}


def expand_query(original_query: str) -> list:
    """
    Generate multiple query variations for better recall using GPT-4o-mini.
    
    Args:
        original_query: User's original question
        
    Returns:
        List of query variations (including original)
    """
    if not OPENAI_API_KEY:
        return [original_query]
    
    try:
        openai.api_key = OPENAI_API_KEY
        
        prompt = f"""You are an expert in CS Joseph's MBTI/Jungian typology system.
Generate 3-4 alternative phrasings of this question to improve search recall.
Use different terminology, synonyms, and related concepts.

Original Question: "{original_query}"

Rules:
- Keep the core intent
- Use CS Joseph terminology (shadow, subconscious, octagram, etc.)
- Include related concepts (e.g., "ENFP development" ‚Üí "ENFP Si inferior growth")
- Vary specificity (broader and narrower versions)

Return as JSON array of strings:"""
        
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Generate query variations for MBTI search. Return JSON array."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Some creativity but consistent
            max_tokens=300
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Try to extract JSON if wrapped in markdown
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        variations = json.loads(response_text)
        
        # Validate it's a list
        if not isinstance(variations, list):
            print(f"‚ö†Ô∏è [QUERY-EXPANSION] GPT returned non-list: {type(variations)}")
            return [original_query]
        
        # Add original query
        all_queries = [original_query] + variations
        print(f"üîç [QUERY-EXPANSION] Expanded to {len(all_queries)} queries: {all_queries}")
        
        return all_queries[:5]  # Cap at 5 total (original + 4 variations)
        
    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è [QUERY-EXPANSION] JSON parsing failed: {e}")
        return [original_query]
    except Exception as e:
        print(f"‚ö†Ô∏è [QUERY-EXPANSION] Query expansion failed: {e}")
        return [original_query]


def rerank_with_gpt(query: str, chunks: list, top_k: int = 10) -> list:
    """
    Re-rank chunks using GPT-4o-mini cross-encoder style scoring.
    
    Args:
        query: User's query
        chunks: Retrieved chunks with scores (10-30 chunks)
        top_k: How many to return after re-ranking
        
    Returns:
        Re-ranked chunks (top_k) with hybrid scores
    """
    if not OPENAI_API_KEY or not chunks:
        return chunks[:top_k]
    
    try:
        openai.api_key = OPENAI_API_KEY
        
        # Build prompt with query and chunk texts
        prompt = f"""You are an expert at judging document relevance for MBTI/Jungian typology questions.

Question: "{query}"

Rank these chunks by relevance (1-10 scale, 10 = perfect match):

"""
        
        for i, chunk in enumerate(chunks[:20], 1):  # Limit to top 20 for GPT context
            text = chunk.get('text', '')[:500]  # First 500 chars per chunk
            prompt += f"Chunk {i}:\n{text}\n\n"
        
        prompt += "Your response (JSON array of scores, e.g., [8, 5, 9, 2, ...]):"
        
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Rank document relevance. Return JSON array of scores."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=100
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Try to extract JSON if wrapped in markdown
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        
        scores = json.loads(response_text)
        
        # Validate it's a list of numbers
        if not isinstance(scores, list):
            print(f"‚ö†Ô∏è [RE-RANK] GPT returned non-list: {type(scores)}")
            return chunks[:top_k]
        
        # Combine with original similarity scores (weighted average)
        chunks_to_rank = chunks[:min(len(scores), len(chunks))]
        for i, chunk in enumerate(chunks_to_rank):
            if i < len(scores):
                gpt_score = scores[i]
                similarity_score = chunk.get('score', chunk.get('boosted_score', 0.0))
                
                # Hybrid score: 40% similarity + 60% GPT relevance
                chunk['rerank_score'] = gpt_score
                chunk['hybrid_score'] = (similarity_score * 0.4) + (gpt_score / 10 * 0.6)
            else:
                # No GPT score for this chunk, use original
                chunk['hybrid_score'] = chunk.get('score', chunk.get('boosted_score', 0.0))
        
        # Sort by hybrid score
        reranked = sorted(chunks_to_rank, key=lambda x: x.get('hybrid_score', 0), reverse=True)
        
        print(f"‚ö° [RE-RANK] Re-ranked {len(chunks_to_rank)} chunks, top hybrid score: {reranked[0].get('hybrid_score', 0):.3f}")
        
        return reranked[:top_k]
        
    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è [RE-RANK] JSON parsing failed: {e}")
        return chunks[:top_k]
    except Exception as e:
        print(f"‚ö†Ô∏è [RE-RANK] Re-ranking failed: {e}")
        return chunks[:top_k]


def query_innerverse_local(question: str) -> str:
    """
    IMPROVED HYBRID SEARCH for MBTI content:
    - Upgraded to text-embedding-3-large for better semantic matching
    - Increased top_k from 10 to 30 for broader initial retrieval
    - Smart query rewriting with MBTI ontology
    - Metadata filtering for type-specific queries
    - Re-ranking for relevance
    """
    try:
        print(f"\nüîç [CLAUDE DEBUG] Query: '{question}'")
        print(f"üìç [CLAUDE DEBUG] Using Pinecone index: {PINECONE_INDEX}")
        print(f"üîë [CLAUDE DEBUG] OpenAI API Key: {'‚úÖ SET' if OPENAI_API_KEY else '‚ùå MISSING'}")
        print(f"üîë [CLAUDE DEBUG] Pinecone API Key: {'‚úÖ SET' if PINECONE_API_KEY else '‚ùå MISSING'}")
        
        if not OPENAI_API_KEY:
            print("‚ùå [CLAUDE DEBUG] OpenAI API key missing!")
            return ""
        
        openai.api_key = OPENAI_API_KEY
        pinecone_index = get_pinecone_index()
        if not pinecone_index:
            print("‚ùå [CLAUDE DEBUG] Failed to get Pinecone index!")
            return ""
        
        print(f"‚úÖ [CLAUDE DEBUG] Pinecone index connected successfully")
        
        # FEATURE #1: Extract metadata filters from query
        metadata_filters = extract_filters_from_query(question)
        
        # FEATURE #2: GPT-powered query expansion for better recall
        search_queries = expand_query(question)
        
        print(f"üîç [QUERY-EXPANSION] Generated {len(search_queries)} query variations:")
        for i, q in enumerate(search_queries, 1):
            print(f"   {i}. '{q}'")
        
        # IMPROVEMENT 2: Broader initial retrieval (top_k=30) for better coverage
        all_chunks = {}  # Deduplicate by text
        
        for query_idx, query in enumerate(search_queries, 1):
            # Get embedding with UPGRADED model
            print(f"üßÆ [CLAUDE DEBUG] Creating embedding for query #{query_idx} with text-embedding-3-large...")
            response = openai.embeddings.create(
                input=query,
                model="text-embedding-3-large"  # UPGRADED from ada-002
            )
            query_vector = response.data[0].embedding
            print(f"‚úÖ [CLAUDE DEBUG] Embedding created: {len(query_vector)} dimensions")
            
            # Query Pinecone with INCREASED top_k for hybrid approach + metadata filters
            query_params = {
                "vector": query_vector,
                "top_k": 30,
                "include_metadata": True
            }
            
            # FEATURE #1: Apply metadata filters if extracted
            if metadata_filters:
                query_params["filter"] = metadata_filters
                print(f"üéØ [METADATA-FILTER] Applying filters to query #{query_idx}: {metadata_filters}")
            
            print(f"üì° [CLAUDE DEBUG] Querying Pinecone with top_k=30...")
            try:
                import asyncio
                from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
                
                # Wrap Pinecone query with 10-second timeout
                with ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        pinecone_index.query,
                        **query_params
                    )
                    query_response = future.result(timeout=10.0)
            except (FuturesTimeoutError, TimeoutError) as e:
                print(f"‚è±Ô∏è [CLAUDE DEBUG] Pinecone query #{query_idx} timed out after 10 seconds")
                continue  # Skip this query and try next one
            
            # Extract and deduplicate contexts
            try:
                matches = query_response.matches
            except AttributeError:
                matches = query_response.get("matches", [])
            
            print(f"üìä [CLAUDE DEBUG] Query #{query_idx} returned {len(matches)} matches")
            if matches:
                print(f"   Top match score: {matches[0].score:.4f}")
                print(f"   Lowest match score: {matches[-1].score:.4f}")
            
            # Extract ALL metadata (including 10 enriched fields)
            enriched_results = extract_all_metadata(matches)
            
            for result in enriched_results:
                text = result['text']
                if text and text not in all_chunks:
                    all_chunks[text] = result
        
        if not all_chunks:
            print("‚ùå [CLAUDE DEBUG] No chunks found! Returning empty message.")
            return "No relevant MBTI content found in knowledge base."
        
        # IMPROVEMENT 3: Metadata-boosted re-ranking
        # Sort by base score, apply metadata boosts, re-sort, take top 12
        sorted_chunks = sorted(all_chunks.values(), key=lambda x: x["score"], reverse=True)[:20]  # Get top 20 first
        
        print(f"üìö [CLAUDE DEBUG] Total unique chunks collected: {len(all_chunks)}")
        print(f"üîÑ [CLAUDE DEBUG] Applying metadata-boosted re-ranking...")
        
        # Apply intelligent re-ranking
        reranked_chunks = rerank_chunks_with_metadata(sorted_chunks, question)
        
        # FEATURE #5: GPT-powered re-ranking for final precision boost
        print(f"‚ö° [CLAUDE DEBUG] Applying GPT-powered re-ranking to top 20 chunks...")
        gpt_reranked_chunks = rerank_with_gpt(question, reranked_chunks[:20], top_k=12)
        final_chunks = gpt_reranked_chunks  # Top 12 after GPT re-ranking
        
        # Log boost details
        boosted_count = sum(1 for c in final_chunks if c.get('boost_applied', 0) > 0)
        print(f"üìà [CLAUDE DEBUG] {boosted_count}/{len(final_chunks)} chunks received metadata boost")
        if boosted_count > 0:
            avg_boost = sum(c.get('boost_applied', 0) for c in final_chunks) / len(final_chunks)
            print(f"üìà [CLAUDE DEBUG] Average boost: +{avg_boost:.3f}")
        
        print(f"üìö [CLAUDE DEBUG] Top 12 chunks selected for context")
        print(f"üìö [CLAUDE DEBUG] Sample sources: {', '.join(set([c.get('season', 'Unknown') for c in final_chunks[:5] if c.get('season')]))}")
        
        # FEATURE #4: Calculate confidence score
        confidence = calculate_confidence_score(final_chunks, question)
        print(f"üìä [CONFIDENCE] {confidence['stars']} {confidence['level']} ({confidence['score']:.2f}) - {confidence['reasoning']}")
        
        # FEATURE #4: Format citations for display
        citations_text = format_citations(final_chunks)
        
        # Build structured citations data for frontend
        citations_data = {
            "sources": [
                {
                    "season": chunk.get('season', chunk.get('metadata', {}).get('season', 'Unknown')),
                    "filename": chunk.get('filename', chunk.get('metadata', {}).get('filename', 'Unknown')),
                    "score": chunk.get('score', chunk.get('boosted_score', 0.0))
                }
                for chunk in final_chunks[:5]  # Top 5 sources
            ],
            "confidence": {
                "level": confidence['level'],
                "score": confidence['score'],
                "stars": confidence['stars'],
                "reasoning": confidence['reasoning']
            }
        }
        
        # Format with professional structure
        result = format_rag_context_professional(final_chunks)
        
        # Append confidence and citations to context (Claude will include in response)
        result += f"\n\n---\n**Retrieval Confidence:** {confidence['stars']} {confidence['level'].replace('_', ' ').title()} *{confidence['reasoning']}*\n**Sources:**\n{citations_text}"
        
        print(f"‚úÖ [CLAUDE DEBUG] Returning structured context ({len(result)} chars)")
        
        # Return tuple: (context_string, citations_data)
        return result, citations_data
        
    except Exception as e:
        print(f"‚ùå [CLAUDE DEBUG] Pinecone query error: {str(e)}")
        import traceback
        print(f"‚ùå [CLAUDE DEBUG] Full traceback:\n{traceback.format_exc()}")
        return ""

def search_web_brave(query: str) -> str:
    """Search the web using Brave Search API for current information"""
    import httpx
    
    if not BRAVE_API_KEY:
        print("‚ö†Ô∏è BRAVE_API_KEY not set, web search unavailable")
        return "Web search is not configured. Please add a Brave Search API key."
    
    try:
        print(f"üåê Searching web via Brave API: {query}")
        
        headers = {
            'X-Subscription-Token': BRAVE_API_KEY,
            'Accept': 'application/json'
        }
        
        params = {
            'q': query,
            'count': 5,
            'text_decorations': False,
            'search_lang': 'en'
        }
        
        response = httpx.get(
            'https://api.search.brave.com/res/v1/web/search',
            headers=headers,
            params=params,
            timeout=10.0
        )
        
        if response.status_code == 200:
            data = response.json()
            results = []
            
            # Extract web results
            web_results = data.get('web', {}).get('results', [])
            
            for result in web_results[:5]:
                title = result.get('title', '')
                description = result.get('description', '')
                url = result.get('url', '')
                
                if description:
                    results.append(f"{title}\n{description}\nSource: {url}")
            
            if results:
                print(f"‚úÖ Brave Search found {len(results)} results")
                return "\n\n---\n\n".join(results)
            else:
                return f"No web results found for '{query}'."
        
        elif response.status_code == 429:
            return "Rate limit reached. Please try again in a moment."
        
        elif response.status_code == 401:
            return "API key invalid. Please check your Brave Search API key."
        
        else:
            print(f"‚ùå Brave API error {response.status_code}: {response.text}")
            return f"Web search temporarily unavailable (Error {response.status_code})."
    
    except Exception as e:
        print(f"‚ùå Web search error: {str(e)}")
        return "Unable to perform web search at this time."

def make_claude_api_call_with_retry(client, **kwargs):
    """
    Make Claude API call with exponential backoff retry logic for overloaded errors.
    Retries: 2s wait, then 4s wait. Max 3 attempts total.
    """
    max_retries = 2
    retry_delays = [2, 4]  # Exponential backoff: 2s, then 4s
    
    for attempt in range(max_retries + 1):
        try:
            response = client.messages.create(**kwargs)
            return response
        
        except anthropic.InternalServerError as e:
            # Check if it's an overloaded error
            error_message = str(e).lower()
            is_overloaded = 'overloaded' in error_message
            
            if is_overloaded and attempt < max_retries:
                wait_time = retry_delays[attempt]
                print(f"‚ö†Ô∏è Claude API overloaded (attempt {attempt + 1}/{max_retries + 1}). Retrying in {wait_time}s...")
                time.sleep(wait_time)
                continue
            
            # If not overloaded or max retries reached, re-raise
            if is_overloaded:
                raise Exception("Claude API is temporarily busy. Please try again in a moment.")
            else:
                raise
        
        except Exception as e:
            # For other errors, don't retry - just raise immediately
            raise
    
    # This shouldn't be reached, but just in case
    raise Exception("Claude API is temporarily busy. Please try again in a moment.")

def chat_with_claude(messages: List[Dict[str, str]], conversation_id: int) -> tuple[str, List[Dict]]:
    """
    Send messages to Claude and get response with automatic InnerVerse backend queries
    Returns: (response_text, tool_use_details)
    """
    if not ANTHROPIC_API_KEY:
        raise Exception("ANTHROPIC_API_KEY not set")
    
    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    
    tools = [
        {
            "name": "query_reference_data",
            "description": "Get exact MBTI type structures like four sides mappings, cognitive function stacks, temperaments, and quadra assignments. Use this FIRST for factual lookup questions about type structures (e.g., 'What are INFJ's four sides?', 'ENFP function stack', 'INTJ temperament'). Returns verified reference data.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "type_code": {
                        "type": "string",
                        "description": "The MBTI type code in uppercase (e.g., INFJ, ENFP, ISTJ, ENTP)"
                    }
                },
                "required": ["type_code"]
            }
        },
        {
            "name": "query_innerverse_backend",
            "description": "Search the InnerVerse knowledge base containing 183+ CS Joseph YouTube transcripts on MBTI, Jungian psychology, cognitive functions, and type theory. Use this when the user asks MBTI/psychology questions that need examples, context, or detailed explanations beyond basic type structures.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "The question to search for in the MBTI knowledge base."
                    }
                },
                "required": ["question"]
            }
        },
        {
            "name": "search_web",
            "description": "Search the web for current information, facts, news, or general knowledge not in the MBTI knowledge base. Use this for restaurants, locations, current events, general facts, etc.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query for public information"
                    }
                },
                "required": ["query"]
            }
        }
    ]
    
    # Build system prompt with all 3 layers using centralized prompt builder
    # This ensures reference data injection is structurally enforced
    last_user_message_content = None
    for msg in reversed(messages):
        if msg.get('role') == 'user':
            last_user_message_content = msg.get('content')
            break
    
    try:
        system_message, prompt_metadata = build_system_prompt(
            conversation_id=conversation_id,
            user_message=last_user_message_content or ""
        )
    except PromptAssemblyError as e:
        error_msg = f"Prompt assembly failed: {e}"
        print(f"‚ùå [PROMPT BUILDER] {error_msg}")
        return (error_msg, [])
    
    tool_use_details = []
    max_iterations = 3
    
    for iteration in range(max_iterations):
        try:
            response = make_claude_api_call_with_retry(
                client,
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                system=system_message,
                tools=tools,
                messages=messages,
                timeout=60.0  # 60-second timeout to prevent hanging
            )
        except Exception as e:
            # Catch user-friendly error messages from retry logic
            error_msg = str(e)
            if "temporarily busy" in error_msg.lower():
                # Return the user-friendly message to the frontend
                return (error_msg, tool_use_details, None)
            else:
                # Re-raise other unexpected errors
                raise
        
        # Log Claude API usage
        if hasattr(response, 'usage'):
            input_tokens = getattr(response.usage, 'input_tokens', 0)
            output_tokens = getattr(response.usage, 'output_tokens', 0)
            cost = (input_tokens / 1000 * 0.003) + (output_tokens / 1000 * 0.015)
            
            # Import log function from main.py
            try:
                from main import log_api_usage
                log_api_usage("claude_chat", "claude-sonnet-4", input_tokens, output_tokens, cost)
            except Exception as e:
                print(f"‚ö†Ô∏è Could not log Claude usage: {e}")
        
        if response.stop_reason == "end_turn":
            for block in response.content:
                if hasattr(block, 'text'):
                    # Extract follow-up question if present
                    full_text = block.text
                    follow_up_question = extract_follow_up_question(full_text)
                    
                    # Remove the [FOLLOW-UP: ...] from the main text
                    main_text = full_text
                    if follow_up_question:
                        import re
                        follow_up_match = re.search(r'\[FOLLOW-UP:\s*(.+?)\]', full_text, re.IGNORECASE)
                        if follow_up_match:
                            main_text = full_text[:follow_up_match.start()].strip()
                    
                    return (main_text, tool_use_details, follow_up_question)
        
        elif response.stop_reason == "tool_use":
            for block in response.content:
                if block.type == "tool_use":
                    tool_name = block.name
                    tool_input = block.input
                    
                    if tool_name == "query_reference_data":
                        type_code = tool_input.get("type_code", "").upper()
                        print(f"üìñ [REFERENCE DATA] Looking up type: {type_code}")
                        
                        # Lookup in reference data using same loader as type_injection
                        type_data = get_type_stack(type_code)
                        
                        if type_data:
                            # Extract four sides data properly
                            four_sides = type_data.get("four_sides", {})
                            
                            # Extract type codes from each side
                            ego_type = four_sides.get('ego', {}).get('type', 'Unknown')
                            shadow_type = four_sides.get('shadow', {}).get('type', 'Unknown')
                            subconscious_type = four_sides.get('subconscious', {}).get('type', 'Unknown')
                            superego_type = four_sides.get('superego', {}).get('type', 'Unknown')
                            
                            # Format functions for each side
                            def format_functions(funcs):
                                return '\n'.join([f"  ‚Ä¢ {f.get('position', 'Unknown')}: {f.get('function', 'Unknown')}" for f in funcs])
                            
                            # Build formatted response
                            result_text = f"""**{type_code} Complete Type Information:**

üé≠ **Ego ({ego_type}):**
{format_functions(four_sides.get('ego', {}).get('functions', []))}

üë• **Shadow ({shadow_type}):**
{format_functions(four_sides.get('shadow', {}).get('functions', []))}

üîÑ **Subconscious ({subconscious_type}):**
{format_functions(four_sides.get('subconscious', {}).get('functions', []))}

‚ö° **Superego ({superego_type}):**
{format_functions(four_sides.get('superego', {}).get('functions', []))}

**Categories:**
‚Ä¢ Temperament: {type_data.get('categories', {}).get('temperament', 'Unknown')}
‚Ä¢ Quadra: {type_data.get('categories', {}).get('quadra', 'Unknown')}
‚Ä¢ Interaction Style: {type_data.get('categories', {}).get('interaction_style', 'Unknown')}
‚Ä¢ Temple: {type_data.get('categories', {}).get('temple', 'Unknown')}"""
                            
                            print(f"‚úÖ [REFERENCE DATA] Found and formatted data for {type_code}")
                        else:
                            result_text = f"No reference data found for type: {type_code}"
                            print(f"‚ùå [REFERENCE DATA] No data for {type_code}")
                        
                        tool_use_details.append({
                            "tool": "query_reference_data",
                            "type_code": type_code,
                            "found": bool(type_data)
                        })
                        
                        messages.append({
                            "role": "assistant",
                            "content": response.content
                        })
                        
                        messages.append({
                            "role": "user",
                            "content": [
                                {
                                    "type": "tool_result",
                                    "tool_use_id": block.id,
                                    "content": result_text
                                }
                            ]
                        })
                    
                    elif tool_name == "query_innerverse_backend":
                        question = tool_input.get("question", "")
                        print(f"üîç Querying InnerVerse Pinecone (local) for: {question}")
                        
                        result = query_innerverse_local(question)
                        # Handle tuple return (context, citations_data) or string (backwards compat)
                        if isinstance(result, tuple):
                            backend_result, _ = result  # Citations not used in non-streaming mode
                        else:
                            backend_result = result
                        
                        tool_use_details.append({
                            "tool": "query_innerverse_backend",
                            "question": question,
                            "result_length": len(backend_result)
                        })
                        
                        messages.append({
                            "role": "assistant",
                            "content": response.content
                        })
                        
                        messages.append({
                            "role": "user",
                            "content": [
                                {
                                    "type": "tool_result",
                                    "tool_use_id": block.id,
                                    "content": backend_result if backend_result else "No relevant content found in knowledge base."
                                }
                            ]
                        })
                    
                    elif tool_name == "search_web":
                        query = tool_input.get("query", "")
                        print(f"üåê Searching web for: {query}")
                        
                        web_result = search_web_brave(query)
                        
                        tool_use_details.append({
                            "tool": "search_web",
                            "query": query,
                            "result_length": len(web_result)
                        })
                        
                        messages.append({
                            "role": "assistant",
                            "content": response.content
                        })
                        
                        messages.append({
                            "role": "user",
                            "content": [
                                {
                                    "type": "tool_result",
                                    "tool_use_id": block.id,
                                    "content": web_result
                                }
                            ]
                        })
            continue
        
        else:
            return "I encountered an error processing your request.", tool_use_details, None
    
    return "I reached the maximum number of processing steps. Please try rephrasing your question.", tool_use_details, None


def chat_with_claude_streaming(messages: List[Dict[str, str]], conversation_id: int):
    """
    Send messages to Claude with STREAMING enabled for real-time response display
    Yields chunks as they arrive from Claude
    RETURNS: Yields SSE events, final event includes follow-up question if present
    """
    import json  # üêõ FIX: Import at function top to prevent UnboundLocalError
    
    if not ANTHROPIC_API_KEY:
        yield "data: " + '{"error": "ANTHROPIC_API_KEY not set"}\n\n'
        return
    
    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    full_response_text = []  # Accumulate response for follow-up extraction
    citations_data = None  # Store citations from RAG query
    
    tools = [
        {
            "name": "query_reference_data",
            "description": "Get exact MBTI type structures like four sides mappings, cognitive function stacks, temperaments, and quadra assignments. Use this FIRST for factual lookup questions about type structures (e.g., 'What are INFJ's four sides?', 'ENFP function stack', 'INTJ temperament'). Returns verified reference data.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "type_code": {
                        "type": "string",
                        "description": "The MBTI type code in uppercase (e.g., INFJ, ENFP, ISTJ, ENTP)"
                    }
                },
                "required": ["type_code"]
            }
        },
        {
            "name": "query_innerverse_backend",
            "description": "Search the InnerVerse knowledge base containing 183+ CS Joseph YouTube transcripts on MBTI, Jungian psychology, cognitive functions, and type theory. Use this when the user asks MBTI/psychology questions that need examples, context, or detailed explanations beyond basic type structures.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "The question to search for in the MBTI knowledge base."
                    }
                },
                "required": ["question"]
            }
        },
        {
            "name": "search_web",
            "description": "Search the web for current information, facts, news, or general knowledge not in the MBTI knowledge base. Use this for restaurants, locations, current events, general facts, etc.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query for public information"
                    }
                },
                "required": ["query"]
            }
        }
    ]
    
    # Build system prompt with all 3 layers using centralized prompt builder
    # This ensures reference data injection is structurally enforced
    last_user_message_content = None
    for msg in reversed(messages):
        if msg.get('role') == 'user':
            last_user_message_content = msg.get('content')
            break
    
    try:
        system_message, prompt_metadata = build_system_prompt(
            conversation_id=conversation_id,
            user_message=last_user_message_content or ""
        )
    except PromptAssemblyError as e:
        error_msg = f"Prompt assembly failed: {e}"
        print(f"‚ùå [PROMPT BUILDER] {error_msg}")
        yield "data: " + json.dumps({"error": error_msg}) + "\n\n"
        return
    
    max_iterations = 3
    
    try:
        for iteration in range(max_iterations):
            # Send search status to frontend
            if iteration > 0:
                yield "data: " + '{"status": "searching"}\n\n'
            
            with client.messages.stream(
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                system=system_message,
                tools=tools,
                messages=messages,
                timeout=60.0  # 60-second timeout to prevent hanging
            ) as stream:
                for event in stream:
                    if event.type == "content_block_start":
                        continue
                        
                    elif event.type == "content_block_delta":
                        if hasattr(event.delta, "text"):
                            # Stream text chunks to frontend immediately (no batching for max speed)
                            text_chunk = event.delta.text
                            full_response_text.append(text_chunk)  # Accumulate for follow-up extraction
                            yield "data: " + json.dumps({"chunk": text_chunk}) + "\n\n"
                            
                    elif event.type == "message_stop":
                        # Check if we hit a tool use
                        final_message = stream.get_final_message()
                        
                        if final_message.stop_reason == "tool_use":
                            # Handle tool use (Reference data, Pinecone search, or web search)
                            for block in final_message.content:
                                if block.type == "tool_use":
                                    if block.name == "query_reference_data":
                                        type_code = block.input.get("type_code", "").upper()
                                        
                                        # Lookup reference data using same loader as type_injection
                                        yield "data: " + '{"status": "looking_up_reference"}\n\n'
                                        type_data = get_type_stack(type_code)
                                        
                                        if type_data:
                                            result_text = json.dumps(type_data, indent=2)
                                            print(f"‚úÖ [REFERENCE DATA STREAMING] Found data for {type_code} via type_injection.get_type_stack()")
                                        else:
                                            result_text = f"No reference data found for type: {type_code}"
                                            print(f"‚ùå [REFERENCE DATA STREAMING] No data for {type_code} in reference_data.json")
                                        
                                        # Add tool result to messages and continue streaming
                                        messages.append({
                                            "role": "assistant",
                                            "content": final_message.content
                                        })
                                        
                                        messages.append({
                                            "role": "user",
                                            "content": [
                                                {
                                                    "type": "tool_result",
                                                    "tool_use_id": block.id,
                                                    "content": result_text
                                                }
                                            ]
                                        })
                                        break
                                    
                                    elif block.name == "query_innerverse_backend":
                                        question = block.input.get("question", "")
                                        
                                        # Query Pinecone locally (FAST!)
                                        yield "data: " + '{"status": "searching_pinecone"}\n\n'
                                        result = query_innerverse_local(question)
                                        
                                        # Handle tuple return (context, citations_data) or string (backwards compat)
                                        if isinstance(result, tuple):
                                            backend_result, citations_data = result
                                        else:
                                            backend_result = result
                                            citations_data = None
                                        
                                        # Add tool result to messages and continue streaming
                                        messages.append({
                                            "role": "assistant",
                                            "content": final_message.content
                                        })
                                        
                                        messages.append({
                                            "role": "user",
                                            "content": [
                                                {
                                                    "type": "tool_result",
                                                    "tool_use_id": block.id,
                                                    "content": backend_result if backend_result else "No relevant content found in knowledge base."
                                                }
                                            ]
                                        })
                                        break
                                        
                                    elif block.name == "search_web":
                                        query = block.input.get("query", "")
                                        
                                        # Search web with Brave API
                                        yield "data: " + '{"status": "searching_web"}\n\n'
                                        web_result = search_web_brave(query)
                                        
                                        # Add tool result to messages and continue streaming
                                        messages.append({
                                            "role": "assistant",
                                            "content": final_message.content
                                        })
                                        
                                        messages.append({
                                            "role": "user",
                                            "content": [
                                                {
                                                    "type": "tool_result",
                                                    "tool_use_id": block.id,
                                                    "content": web_result
                                                }
                                            ]
                                        })
                                        break
                            # Continue to next iteration to get final response with context
                            continue
                        else:
                            # Done! Log usage and extract follow-up question
                            import json
                            
                            # Log Claude API usage
                            if hasattr(final_message, 'usage'):
                                input_tokens = getattr(final_message.usage, 'input_tokens', 0)
                                output_tokens = getattr(final_message.usage, 'output_tokens', 0)
                                cost = (input_tokens / 1000 * 0.003) + (output_tokens / 1000 * 0.015)
                                
                                try:
                                    from main import log_api_usage
                                    log_api_usage("claude_chat_stream", "claude-sonnet-4", input_tokens, output_tokens, cost)
                                    print(f"üí∞ Logged streaming Claude usage: ${cost:.6f}")
                                except Exception as e:
                                    print(f"‚ö†Ô∏è Could not log streaming Claude usage: {e}")
                            
                            follow_up = extract_follow_up_question("".join(full_response_text))
                            done_payload = {"done": True, "follow_up": follow_up}
                            if citations_data:
                                done_payload["citations"] = citations_data
                            yield "data: " + json.dumps(done_payload) + "\n\n"
                            return
    
        # Max iterations reached - send done with follow-up
        follow_up = extract_follow_up_question("".join(full_response_text))
        done_payload = {"done": True, "follow_up": follow_up}
        if citations_data:
            done_payload["citations"] = citations_data
        yield "data: " + json.dumps(done_payload) + "\n\n"
    
    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Claude streaming error: {error_msg}")
        yield "data: " + json.dumps({"error": f"Sorry, I encountered an error: {error_msg}. Please try again."}) + "\n\n"
