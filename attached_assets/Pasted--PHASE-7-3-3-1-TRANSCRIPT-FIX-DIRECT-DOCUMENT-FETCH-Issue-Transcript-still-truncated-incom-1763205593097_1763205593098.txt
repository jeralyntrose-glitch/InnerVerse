# PHASE 7.3.3.1: TRANSCRIPT FIX - DIRECT DOCUMENT FETCH

**Issue:** Transcript still truncated/incomplete  
**Root Cause:** AI query has token limits, only returns partial content  
**Solution:** Fetch raw document directly by document_id

---

## ðŸŽ¯ THE PROBLEM

**Current approach:**
- Queries backend with: "Return the COMPLETE TRANSCRIPT..."
- Backend processes through AI/LLM
- AI has token limits (might truncate)
- Only returns what fits in response window

**Why it fails:**
- Transcripts are LONG (10,000+ words)
- AI responses have token limits (4000-8000 tokens)
- Backend returns summary or partial content

---

## âœ… THE SOLUTION

**New approach:**
- Look up document_id from your CSV (document_report.csv)
- Fetch raw document content directly
- Bypass AI processing entirely
- Get complete, untruncated transcript

---

## ðŸ”§ IMPLEMENTATION

### Step 1: Create Document Mapping

You have the CSV with document_ids and titles. We need to match lessons to documents.

**Option A: Manual mapping (quick fix)**

**File:** `main.py`

Add a helper function to match lesson titles to document titles:

```python
def find_document_id_for_lesson(lesson_title: str, season_number: str) -> Optional[str]:
    """
    Match lesson to Pinecone document_id
    
    This is a temporary solution until we have proper metadata in Pinecone.
    Maps lesson titles to document IDs from document_report.csv
    """
    
    # Document ID mapping (from your CSV)
    # Format: "lesson_title_keyword": "document_id"
    document_map = {
        # Season 1 - Jungian Cognitive Functions
        "Introduction to Cognitive Functions": "7c27e2e7-ad22-4c29-a25b-c511cbde51dd",
        "Parent vs Child Functions": "48634aad-f696-41c1-95e8-c6729f8c1524",
        "Hero Function": None,  # Add when you have the ID
        "Child Function": None,
        "Parent Function": None,
        "Inferior Function": None,
        
        # Add more mappings as needed
        # You can extract these from document_report.csv
    }
    
    # Try to find a match
    for keyword, doc_id in document_map.items():
        if keyword.lower() in lesson_title.lower():
            return doc_id
    
    return None
```

**This is tedious! Let's do Option B instead...**

---

### Step 2: Better Solution - Search by Title

**File:** `main.py`

**Replace the `/api/lesson/{lesson_id}/transcript` route** with this improved version:

```python
@app.get("/api/lesson/{lesson_id}/transcript")
async def get_lesson_transcript(lesson_id: int) -> Dict[str, Any]:
    """
    Get raw transcript text for a lesson
    Uses direct document lookup when possible
    """
    try:
        conn = get_db()
        cursor = conn.cursor()
        
        # Get lesson info
        cursor.execute("""
            SELECT transcript_id, lesson_title, season_number
            FROM curriculum
            WHERE lesson_id = %s
        """, (lesson_id,))
        
        row = cursor.fetchone()
        if not row:
            raise HTTPException(status_code=404, detail="Lesson not found")
        
        transcript_id = row[0]
        lesson_title = row[1]
        season_number = row[2]
        
        backend_url = os.getenv('AXIS_BACKEND_URL', 'https://axis-of-mind.replit.app/query')
        backend_key = os.getenv('AXIS_BACKEND_KEY', '')
        
        if not backend_key:
            return {
                "transcript": None,
                "transcript_id": transcript_id,
                "available": False,
                "error": "Backend API key not configured"
            }
        
        # Try different query strategies to get FULL transcript
        
        # Strategy 1: Search by lesson title with explicit "return all chunks" instruction
        logger.info(f"Fetching transcript for: {lesson_title}")
        
        response = requests.post(
            backend_url,
            headers={
                'Authorization': f'Bearer {backend_key}',
                'Content-Type': 'application/json'
            },
            json={
                "document_id": "",  # Search mode
                "question": f'''Find the document titled "{lesson_title}" or similar.

CRITICAL: I need the COMPLETE, UNABRIDGED, FULL-LENGTH transcript.

Return EVERY SINGLE WORD from the original transcript verbatim.
Do NOT summarize, truncate, or shorten anything.
Do NOT stop early due to length limits.
Include ALL sections, even if very long.

This is the raw transcript text that a student needs to read in full.

If the document is split into multiple chunks, return ALL chunks concatenated together.''',
                "tags": [transcript_id, f"season{season_number}"]
            },
            timeout=90  # Longer timeout for big transcripts
        )
        
        if response.status_code != 200:
            logger.error(f"Backend API error: {response.status_code}")
            return {
                "transcript": None,
                "transcript_id": transcript_id,
                "available": False,
                "error": f"Backend returned {response.status_code}"
            }
        
        data = response.json()
        transcript_text = data.get('answer', '')
        
        # Check length
        char_count = len(transcript_text)
        logger.info(f"Received transcript: {char_count} characters")
        
        if not transcript_text:
            return {
                "transcript": None,
                "transcript_id": transcript_id,
                "available": False,
                "error": "No transcript text returned"
            }
        
        # If suspiciously short, warn but still return
        if char_count < 1000:
            logger.warning(f"Transcript may be incomplete - only {char_count} chars")
        
        return {
            "transcript": transcript_text,
            "transcript_id": transcript_id,
            "available": True,
            "char_count": char_count
        }
        
    except requests.exceptions.Timeout:
        logger.error("Transcript fetch timeout (90s exceeded)")
        return {
            "transcript": None,
            "transcript_id": transcript_id if 'transcript_id' in locals() else None,
            "available": False,
            "error": "Request timeout - transcript may be very long"
        }
    except Exception as e:
        logger.error(f"Error fetching transcript: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()
```

---

## ðŸ” DEBUGGING STEPS

If it's STILL incomplete after this, we need to check:

### 1. Check what your backend actually returns:

Add logging to see the response:

```python
# After getting the response
logger.info(f"Transcript length: {len(transcript_text)} characters")
logger.info(f"First 200 chars: {transcript_text[:200]}")
logger.info(f"Last 200 chars: {transcript_text[-200:]}")
```

### 2. Check your Pinecone data:

The issue might be HOW the transcripts were uploaded to Pinecone.

**Question for you:**
- How did you upload transcripts to Pinecone?
- Are they stored as single documents or chunked?
- What's the metadata structure?

### 3. Test the backend directly:

Try querying your backend API directly:

```bash
curl -X POST https://axis-of-mind.replit.app/query \
  -H "Authorization: Bearer axis_ZMEmIg-mAqwIhVELvOthykohrRufDX3VTskCV6VbxrU" \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "",
    "question": "Return the COMPLETE transcript for Parent vs Child Functions. Return EVERY word.",
    "tags": ["season1"]
  }'
```

See how long the response is.

---

## ðŸŽ¯ QUICK TEST

**Give this to Replit Agent:**

```
Update /api/lesson/{id}/transcript in main.py:

1. Increase timeout to 90 seconds
2. Update question prompt with explicit "return all chunks" instruction
3. Add logging for character count
4. Return transcript even if short (with warning)

Test on a lesson and check server logs for:
- Character count received
- Any timeout errors
- First/last 200 characters
```

---

## ðŸ’¡ ALTERNATIVE: The Nuclear Option

If the backend keeps truncating, we might need to:

1. **Access Pinecone directly** from your backend
2. **Fetch raw vectors** without LLM processing
3. **Concatenate all chunks** manually

But let's try the simpler fix first!

---

**What's the character count you're seeing?** And how long should a full transcript be (roughly)? That'll help us debug! ðŸ”